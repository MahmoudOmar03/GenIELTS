{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UvoePDq751c"
      },
      "source": [
        "# DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8TLD7d_8DMm",
        "outputId": "3dff8949-e865-4f4f-e5ff-2296ea3889ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34zDLhcO751j",
        "outputId": "636a7601-fadd-4e06-ccc3-cfb69b1be3a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting phonemizer\n",
            "  Downloading phonemizer-3.3.0-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from phonemizer) (1.5.1)\n",
            "Collecting segments (from phonemizer)\n",
            "  Downloading segments-2.3.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.11/dist-packages (from phonemizer) (25.3.0)\n",
            "Collecting dlinfo (from phonemizer)\n",
            "  Downloading dlinfo-2.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from phonemizer) (4.14.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from segments->phonemizer) (2024.11.6)\n",
            "Collecting csvw>=1.5.6 (from segments->phonemizer)\n",
            "  Downloading csvw-3.5.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.9.0.post0)\n",
            "Collecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.2.0)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.32.3)\n",
            "Collecting language-tags (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting rdflib (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.25.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->csvw>=1.5.6->segments->phonemizer) (1.17.0)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2025.7.14)\n",
            "Downloading phonemizer-3.3.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dlinfo-2.0.0-py3-none-any.whl (3.7 kB)\n",
            "Downloading segments-2.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading csvw-3.5.1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rfc3986, language-tags, rdflib, isodate, dlinfo, colorama, csvw, segments, phonemizer\n",
            "Successfully installed colorama-0.4.6 csvw-3.5.1 dlinfo-2.0.0 isodate-0.7.2 language-tags-1.2.0 phonemizer-3.3.0 rdflib-7.1.4 rfc3986-1.5.0 segments-2.3.0\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Collecting sounddevice\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice) (1.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (2.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, sounddevice\n",
            "Successfully installed jedi-0.19.2 sounddevice-0.5.2\n",
            "Collecting sequence_align\n",
            "  Downloading sequence_align-0.3.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Downloading sequence_align-0.3.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.4/229.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sequence_align\n",
            "Successfully installed sequence_align-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install phonemizer\n",
        "!pip install python-Levenshtein\n",
        "!pip install requests ipywidgets sounddevice soundfile\n",
        "!pip install sequence_align"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_fpYfnp751o",
        "outputId": "420fad9e-b95a-47f1-e705-a722837324d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 4,526 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8,956 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 espeak-ng-data amd64 1.50+dfsg-10ubuntu0.1 [3,956 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng1 amd64 1.50+dfsg-10ubuntu0.1 [207 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 espeak-ng amd64 1.50+dfsg-10ubuntu0.1 [343 kB]\n",
            "Fetched 4,526 kB in 2s (2,663 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install espeak-ng\n",
        "# MIGHT ALSO NEED TO INSTALL FFMPEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1-ZcToJArw4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ4yRC6wA5_N",
        "outputId": "5612fa97-c90a-4fb6-e88b-996dc2236b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5PNVGehXu_2",
        "outputId": "b867fc3e-e447-47e8-fd96-4e4935f9fe38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.7.8 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.7.10 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.7.10 requires tyro, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "nx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.7.1 requires pandas>=2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "geopandas 1.1.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "arviz 0.22.0 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok TTS pydub requests --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxRSVzbp751q"
      },
      "source": [
        "# IELTS SPEAKING RUBIC FOR CONTEXT AND INFO GROUNDING TO THE LLLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d_F0mHV751u"
      },
      "source": [
        "# audio file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nGqO0xn751u"
      },
      "outputs": [],
      "source": [
        "audio = \"/content/drive/MyDrive/KAUST/Summer/Project/Recording.wav\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrD8jeJMB9S7",
        "outputId": "3569122d-432b-443b-a155-d1afbf69defe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall numpy -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "r-ntmm8NB1Zf",
        "outputId": "c3bfdf7e-d906-40dc-cb2f-404ea3f92a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.7.8 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.7.10 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.7.10 requires tyro, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "nx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.7.1 requires pandas>=2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "geopandas 1.1.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "arviz 0.22.0 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "6d7cb54b39314b97b8e0fb2ec5ed3a50",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkupJL2KmekK"
      },
      "source": [
        "# Configrution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDszrkMmmeSO"
      },
      "outputs": [],
      "source": [
        "listeing_rubic = {\n",
        "    \"Grammatical range and accuracy\": (\n",
        "        \"Band‑9 (39‑40/40 correct): Fully understands gist, detail, inference, attitude and purpose in any accent or speed. \"\n",
        "        \"Band‑8 (35‑38): Very good comprehension with only rare slips on nuance or low‑frequency lexis. \"\n",
        "        \"Band‑7 (30‑34): Good grasp of main ideas and most details; occasional difficulty with dense or rapid speech. \"\n",
        "        \"Band‑6 (23‑29): Generally effective in familiar contexts but misses some detail, inference and fast passages. \"\n",
        "        \"Band‑5 (16‑22): Modest comprehension of overall meaning in predictable situations; frequent loss of detail. \"\n",
        "        \"Band‑4 (11‑15): Limited comprehension; can catch basic factual info in slow, clear speech only. \"\n",
        "        \"Band‑3 (6‑10): Very limited comprehension; recognises isolated words and familiar phrases. \"\n",
        "        \"Band‑2 (3‑5): Intermittent comprehension; picks up only a few isolated words despite repetition. \"\n",
        "        \"Band‑1 (0‑2): No functional ability to understand spoken English.\"\n",
        "    )\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4fTgKprpU1G"
      },
      "outputs": [],
      "source": [
        "irubic = {\"Pronunciation\":\n",
        "              [\"Band 1 : Can produce occasional individual words and phonemes that are recognisable, but no overall meaning is conveyed. Unintelligible.\",\"Band 2 : Uses few acceptable phonological features (possibly because sample is insufficient). Overall problems with delivery impair attempts at connected speech. Individual words and phonemes are mainly mispronounced and little meaning is conveyed. Often unintelligible.\"\n",
        "    ,\"Band 3 : Displays some features of band 2, and some, but not all, of the positive features of band 4.\", \"Band 4 : Uses some acceptable phonological features, but the range is limited. Produces some acceptable chunking, but there are frequent lapses in overall rhythm. Attempts to use intonation and stress, but control is limited. Individual words or phonemes are frequently mispronounced, causing lack of clarity. Understanding requires some effort and there may be patches of speech that cannot be understood.\",\n",
        "    \"Band 5 : Displays all the positive features of band 4, and some, but not all, of the positive features of band 6.\", \"Band 6 : Uses a range of phonological features, but control is variable. Chunking is generally appropriate, but rhythm may be affected by a lack of stress-timing and/or a rapid speech rate. Some effective use of intonation and stress, but this is not sustained. Individual words or phonemes may be mispronounced but this causes only occasional lack of clarity. Can generally be understood throughout without much effort.\",\n",
        "    \"Band 7 : Displays all the positive features of band 6, and some, but not all, of the positive features of band 8.\", \"Band 8 : Uses a wide range of phonological features to convey precise and/or subtle meaning. Can sustain appropriate rhythm. Flexible use of stress and intonation across long utterances, despite occasional lapses. Can be easily understood throughout. Accent has minimal effect on intelligibility.\", \"Band 9 : Uses a full range of phonological features to convey precise and/or subtle meaning. Flexible use of features of connected speech is sustained throughout. Can be effortlessly understood throughout. Accent has no effect on intelligibility.\"\n",
        "        ], \"Grammatical range and accuracy\" : [\n",
        "        \"Band 1 : No rateable languageunless memorised.\",\n",
        "        \"Band 2 : No evidence of basic sentence forms.\",\n",
        "        \"Band 3 : Basic sentence forms are attempted but grammatical errors are numerous except in apparently memorised utterances.\",\n",
        "        \"Band 4 : Can produce basic sentence forms and some short utterances are error-free. Subordinate clauses are rare and, overall, turns are short, structures are repetitive and errors are frequent.\",\n",
        "        \"Band 5 : Basic sentence forms are fairly well controlled for accuracy. Complex structures are attempted but these are limited in range, nearly always contain errors and may lead to the need for reformulation.\",\n",
        "        \"Band 6 : Produces a mix of short and complex sentence forms and a variety of structures with limited flexibility. Though errors frequently occur in complex structures, these rarely impede communication.\",\n",
        "        \"Band 7 : A range of structures flexibly used. Error-free sentences are frequent. Both simple and complex sentences are used effectively despite some errors. A few basic errors persist.\",\n",
        "        \"Band 8 : Wide resource, readily and flexibly used to discuss all topics and convey precise meaning. Skilful use of less common and idiomatic items despite occasional inaccuracies in word choice and collocation. Effective use of paraphrase as required.\",\n",
        "        \"Band 9 : Structures are precise and accurate at all times, apart from ‘mistakes’ characteristic of native speaker speech.\"\n",
        "        ], \"Lexical resource\" : [\n",
        "        \"Band 1 : No resource bar a few isolated words. No communication possible.\",\n",
        "        \"Band 2 : Very limited resource. Utterances consist of isolated words or memorised utterances. Little communication possible without the support of mime or gesture.\",\n",
        "        \"Band 3 : Resource limited to simple vocabulary used primarily to convey personal information. Vocabulary inadequate for unfamiliar topics.\",\n",
        "        \"Band 4 : Resource sufficient for familiar topics but only basic meaning can be conveyed on unfamiliar topics. Frequent inappropriaciesand errors in word choice. Rarely attempts paraphrase.\",\n",
        "        \"Band 5 : Resource sufficient to discuss familiar and unfamiliar topics but there is limited flexibility. Attempts paraphrase but not always with success.\",\n",
        "        \"Band 6 : Resource sufficient to discuss topics at length. Vocabulary use may be inappropriate but meaning is clear. Generally able to paraphrase successfully.\",\n",
        "        \"Band 7 : Resource flexibly used to discuss a variety of topics. Some ability to use less common and idiomatic items and an awareness of style and collocation is evident though inappropriaciesoccur. Effective use of paraphrase as required.\",\n",
        "        \"Band 8 : Wide resource, readily and flexibly used to discuss all topics and convey precise meaning. Skilful use of less common and idiomatic items despite occasional inaccuracies in word choice and collocation. Effective use of paraphrase as required.\",\n",
        "        \"Band 9 : Total flexibility and precise use in all contexts. Sustained use of accurate and idiomatic language.\"\n",
        "        ]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPFsLPqZ751v"
      },
      "source": [
        "# Extracting actual phonemes from the audio (in : audio, out : phoneme seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMj3EY6SBlGs"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import TextStreamer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UekyzcJ751v"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "def audio_to_phoneme(audio_file_path,return_logits=False):\n",
        "  # Choose a model checkpoint\n",
        "  model_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\" # Example: multilingual phoneme model\n",
        "\n",
        "  # Load the processor and model\n",
        "  processorp = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "  modelp = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "\n",
        "  # Load audio file\n",
        "  speech, sample_rate = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "\n",
        "  # Resample to 16kHz if needed\n",
        "  if sample_rate != 16000:\n",
        "      speech = librosa.resample(speech, orig_sr=sample_rate, target_sr=16000)\n",
        "\n",
        "  # Prepare input values for the model\n",
        "  # The `return_tensors=\"pt\"` argument ensures PyTorch tensors are returned.\n",
        "  input_values = processorp(speech, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  modelp.to(device)\n",
        "  input_values = input_values.to(device)\n",
        "  with torch.no_grad():\n",
        "      logits = modelp(input_values).logits\n",
        "      if return_logits:\n",
        "        return logits\n",
        "\n",
        "  # Take argmax to get the most probable phoneme IDs\n",
        "  predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "  # Decode the predicted IDs to phoneme string\n",
        "  phoneme_transcription = processorp.batch_decode(predicted_ids)\n",
        "\n",
        "  print(f\"Phoneme Transcription: {phoneme_transcription}\")\n",
        "  phoneme = \" \".join(phoneme_transcription)\n",
        "  return phoneme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FJpK_gC751w"
      },
      "source": [
        "# calculatin the avg Goodness of pronounciation of the entire audio (in : uses the logits of the model, out: GOP score + fluency level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ohd4H2d751x"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def audio_to_phoneme_gop(audio_file_path):\n",
        "  logits = audio_to_phoneme(audio_file_path,return_logits=True)\n",
        "\n",
        "  # Get probabilities\n",
        "  probs = F.softmax(logits, dim=-1)  # shape: (batch, time, vocab)\n",
        "\n",
        "  # Get predicted phoneme IDs\n",
        "  predicted_ids = torch.argmax(probs, dim=-1)\n",
        "\n",
        "  # For each time step, get the probability of the predicted phoneme\n",
        "  batch_idx = 0  # if batch size is 1\n",
        "  phoneme_probs = probs[batch_idx, torch.arange(probs.shape[1]), predicted_ids[batch_idx]]\n",
        "\n",
        "  # Average probability as a simple fluency/confidence score\n",
        "  gop_score = phoneme_probs.mean().item()\n",
        "  print(f\"GOP Score (average confidence): {gop_score:.3f}\")\n",
        "\n",
        "\n",
        "  def fluency_level(gop_score):\n",
        "      if gop_score > 0.9:\n",
        "          return \"High fluency\"\n",
        "      elif gop_score > 0.75:\n",
        "          return \"Moderate fluency\"\n",
        "      else:\n",
        "          return \"Low fluency\"\n",
        "\n",
        "  fluency = fluency_level(gop_score)\n",
        "  return gop_score, fluency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIGmCLa-751y"
      },
      "source": [
        "# loads whisper small to extract transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4GYjy5D751y",
        "outputId": "1f2969da-6dec-4bd7-c056-c4fd808c9fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WhisperForConditionalGeneration(\n",
              "  (model): WhisperModel(\n",
              "    (encoder): WhisperEncoder(\n",
              "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (embed_positions): Embedding(1500, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x WhisperEncoderLayer(\n",
              "          (self_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): WhisperDecoder(\n",
              "      (embed_tokens): Embedding(51864, 768, padding_idx=50256)\n",
              "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x WhisperDecoderLayer(\n",
              "          (self_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (proj_out): Linear(in_features=768, out_features=51864, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "import torchaudio\n",
        "from transformers import AutoTokenizer, AutoModelForSpeechSeq2Seq, AutoProcessor, BitsAndBytesConfig\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_id = \"openai/whisper-small.en\"\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Load processor\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YrExU737515"
      },
      "source": [
        "# extracts transcript of the audio (note: fallback not implemented) (in : audio, out : transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b_jqhKBb751-",
        "outputId": "ec398184-49f6-46b4-c7f1-fd457b82b4fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'transcript = audio_to_text_simple(audio)\\nprint(transcript)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Alternative approach using Transformers pipeline (more robust)\n",
        "from transformers import pipeline\n",
        "\n",
        "def audio_to_text_pipeline(audio_file_path, chunk_length_s=30):\n",
        "    \"\"\"\n",
        "    Convert audio file to text using Transformers pipeline (most reliable method)\n",
        "\n",
        "    Args:\n",
        "        audio_file_path (str): Path to the audio file\n",
        "        chunk_length_s (int): Length of audio chunks in seconds\n",
        "\n",
        "    Returns:\n",
        "        str: Transcribed text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a speech recognition pipeline\n",
        "        pipe = pipeline(\n",
        "            \"automatic-speech-recognition\",\n",
        "            model=model_id,\n",
        "            device=0 if torch.cuda.is_available() else -1,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            chunk_length_s=chunk_length_s,\n",
        "            return_timestamps=True  # Enable timestamps for better alignment\n",
        "        )\n",
        "\n",
        "        # Process the audio file\n",
        "        result = pipe(audio_file_path)\n",
        "\n",
        "        # Extract text from result\n",
        "        if isinstance(result, dict):\n",
        "            return result.get(\"text\", \"\")\n",
        "        elif isinstance(result, list):\n",
        "            return \" \".join([chunk.get(\"text\", \"\") for chunk in result])\n",
        "        else:\n",
        "            return str(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline method failed: {e}\")\n",
        "        print(\"Falling back to manual method...\")\n",
        "        return audio_to_text_chunked(audio_file_path, chunk_length_s=chunk_length_s)\n",
        "\n",
        "def audio_to_text_simple(audio_file_path):\n",
        "    \"\"\"\n",
        "    Simple audio to text conversion using librosa and basic processing\n",
        "\n",
        "    Args:\n",
        "        audio_file_path (str): Path to the audio file\n",
        "\n",
        "    Returns:\n",
        "        str: Transcribed text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try pipeline method first (most reliable)\n",
        "        return audio_to_text_pipeline(audio_file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline failed, trying chunked method: {e}\")\n",
        "        try:\n",
        "            # Fallback to chunked method\n",
        "            return audio_to_text_chunked(audio_file_path)\n",
        "        except Exception as e2:\n",
        "            print(f\"Chunked method failed: {e2}\")\n",
        "            # Final fallback\n",
        "            return audio_to_text(audio_file_path)\n",
        "\n",
        "'''transcript = audio_to_text_simple(audio)\n",
        "print(transcript)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VunFPwEO752B"
      },
      "source": [
        "# extract expected phonemes (in : transcript, out : expected phonemes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SD3u25YA752C",
        "outputId": "18b58b70-18d0-4b50-9eb6-28791ceeaf9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"lexicon, ref_words = generate_reference_phoneme(transcript)\\nreference_phoneme =' '.join([phon for w, phon in lexicon])\\nreference_phoneme\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import phonemizer\n",
        "from phonemizer.punctuation import Punctuation\n",
        "from phonemizer.backend import EspeakBackend\n",
        "from phonemizer.separator import Separator\n",
        "\n",
        "def generate_reference_phoneme(reference_text):\n",
        "    text = Punctuation(';:,.!\"?()').remove(reference_text)\n",
        "    ref_words = [w.lower() for w in text.strip().split(' ') if w]\n",
        "\n",
        "\n",
        "    # initialize the espeak backend for English\n",
        "    backend = EspeakBackend('en-us')\n",
        "\n",
        "    # separate phones by a space and ignoring words boundaries\n",
        "    separator = Separator(phone='', word=None)\n",
        "\n",
        "    # build the lexicon by phonemizing each word one by one. The backend.phonemize\n",
        "    # function expect a list as input and outputs a list.\n",
        "    lexicon = [ (word, backend.phonemize([word], separator=separator, strip=True)[0])\n",
        "        for word in ref_words]\n",
        "\n",
        "    return lexicon, ref_words\n",
        "\n",
        "'''lexicon, ref_words = generate_reference_phoneme(transcript)\n",
        "reference_phoneme =' '.join([phon for w, phon in lexicon])\n",
        "reference_phoneme'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDGXBRxN752C"
      },
      "source": [
        "# aligns the expected and observed phonemes (useful in extracting where the error occured (word level and phoneme level))\n",
        "## in : transcript, expected phonemes, actual phonemes, out : an aligned sequence for each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwlA17PP752D"
      },
      "outputs": [],
      "source": [
        "from sequence_align.pairwise import hirschberg, needleman_wunsch\n",
        "\n",
        "def align_sequences(reference_phoneme, phoneme,transcript):\n",
        "  seq_a = reference_phoneme\n",
        "  seq_b = list(phoneme.replace(' ',''))\n",
        "\n",
        "  # recorded_phoneme['text']\n",
        "  aligned_seq_a, aligned_seq_b = needleman_wunsch(\n",
        "      seq_a,\n",
        "      seq_b,\n",
        "      match_score=1.0,\n",
        "      mismatch_score=-1.0,\n",
        "      indel_score=-1.0,\n",
        "      gap=\"_\",\n",
        "  )\n",
        "  aligned_reference_seq = ''.join(aligned_seq_a)\n",
        "  aligned_recorded_seq = ''.join(aligned_seq_b)\n",
        "\n",
        "  print('Reference Text: ', transcript)\n",
        "  print('Reference Phoneme:',aligned_reference_seq)\n",
        "  print('Recorded Phoneme: ', aligned_recorded_seq)\n",
        "\n",
        "  return aligned_reference_seq, aligned_recorded_seq # Return the aligned sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvhun9Rr752G"
      },
      "source": [
        "# splits the sequence to a list of tuples (word, expected phoneme, actual phoneme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "KzYMFNRt752H",
        "outputId": "e8d8fdb5-5957-4026-f345-aec43159a402"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# recorded_sequence = \"aɪ_hoːp_ðeɪ_hɛv_maɪ_fiːv__rədbrænd_aɪl_biː_bæk_su_n__tʊ_pliːz_w_iːdfoː__miː_\"\\nref_start_positions = find_word_start_positions(\\'\\'.join(aligned_reference_seq))\\n\\n# split recorded based on the reference start positions\\nrec_split_words = split_recorded_sequence(\\'\\'.join(aligned_recorded_seq), ref_start_positions)\\nrec_split_words = [re.sub(\\'( |\\\\_)$\\',\\'\\',w) for w in rec_split_words]\\n\\n# split ref based on the reference start positions\\nref_split_words = split_recorded_sequence(\\'\\'.join(aligned_reference_seq), ref_start_positions)\\nref_split_words = [re.sub(\\'(\\\\_| )$\\',\\'\\',w) for w in ref_split_words]\\n\\n# print(\\'Reference Text: \\',reference_text)\\n# print(\\'(word, reference_phoneme, recorded_phoneme)\\',list(zip(ref_words, ref_split_words, rec_split_words)))\\nword_comparision_list = list(zip(ref_words, ref_split_words, rec_split_words))\\nword_comparision_list'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def find_word_start_positions(reference_sequence):\n",
        "    # Split the sequence into words based on spaces\n",
        "    words = reference_sequence.split()\n",
        "    # Initialize a list to store the start positions\n",
        "    start_positions = []\n",
        "    # Initialize the current position\n",
        "    current_position = 0\n",
        "    # Iterate over the words\n",
        "    for word in words:\n",
        "        # Add the current position to the start positions list\n",
        "        start_positions.append(current_position)\n",
        "        # Increment the current position by the length of the word plus 1 (for the space)\n",
        "        current_position += len(word) + 1\n",
        "    return start_positions\n",
        "\n",
        "def split_recorded_sequence(recorded_sequence, start_positions):\n",
        "    # Initialize a list to store the split words\n",
        "    split_words = []\n",
        "    # Iterate over the start positions\n",
        "    for i in range(len(start_positions)):\n",
        "        # Get the start position\n",
        "        start = start_positions[i]\n",
        "        # If it's the last word, get the end position as the length of the sequence\n",
        "        if i == len(start_positions) - 1:\n",
        "            end = len(recorded_sequence)\n",
        "        # Otherwise, get the end position as the start position of the next word\n",
        "        else:\n",
        "            end = start_positions[i + 1]\n",
        "        # Extract the word from the recorded sequence\n",
        "        word = recorded_sequence[start:end]\n",
        "        # Add the word to the list\n",
        "        split_words.append(word)\n",
        "    return split_words\n",
        "\n",
        "'''# recorded_sequence = \"aɪ_hoːp_ðeɪ_hɛv_maɪ_fiːv__rədbrænd_aɪl_biː_bæk_su_n__tʊ_pliːz_w_iːdfoː__miː_\"\n",
        "ref_start_positions = find_word_start_positions(''.join(aligned_reference_seq))\n",
        "\n",
        "# split recorded based on the reference start positions\n",
        "rec_split_words = split_recorded_sequence(''.join(aligned_recorded_seq), ref_start_positions)\n",
        "rec_split_words = [re.sub('( |\\\\_)$','',w) for w in rec_split_words]\n",
        "\n",
        "# split ref based on the reference start positions\n",
        "ref_split_words = split_recorded_sequence(''.join(aligned_reference_seq), ref_start_positions)\n",
        "ref_split_words = [re.sub('(\\\\_| )$','',w) for w in ref_split_words]\n",
        "\n",
        "# print('Reference Text: ',reference_text)\n",
        "# print('(word, reference_phoneme, recorded_phoneme)',list(zip(ref_words, ref_split_words, rec_split_words)))\n",
        "word_comparision_list = list(zip(ref_words, ref_split_words, rec_split_words))\n",
        "word_comparision_list'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENtLE8RM752M"
      },
      "source": [
        "# loads the llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "c09c22fb7827415986188defddce5823",
            "ae6ecdae20ab48988f143c3f63677208",
            "d31f0df406934d48ba6f00646c98bb23",
            "5d97169e778345808ae59469569ec469",
            "66ea7a99b2354201ab3d2c91398ca099",
            "dd394e3a49e04fd4a815b8390fffaea9",
            "cbf43c15d57f43bc8c245d46edac213d",
            "7cad52837edb48ca9307191d0325b6ea",
            "e991be4fa83e4e3397c080553c55e408",
            "6469e76898ef4257a75e007db2399bbc",
            "d7af522e0cb34a6d953d96499a09b32d"
          ]
        },
        "id": "INk7yPq9752Q",
        "outputId": "1f7d7088-9c1c-4f18-9838-9de591b7cacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-2778502563.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.8: Fast Qwen3 patching. Transformers: 4.53.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c09c22fb7827415986188defddce5823"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 15000,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")\n",
        "\n",
        "# Add a default chat template if it's not set\n",
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgb3dJU8752R"
      },
      "source": [
        "# LLM FEEDBACK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrXKLG6y752R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import TextStreamer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def analyze_pronunciation_with_qwen( transcript, word_comparision_list,rubic, model,fluency,gop_score, tokenizer, max_new_tokens=32):\n",
        "  \"\"\"\n",
        "  For each word, send a prompt to Qwen 3 with audio, transcript, expected and actual phonemes.\n",
        "  Output: If a word has a phoneme error, output only the word and the single incorrect phoneme (no explanation). If the word is correct, output only '1'.\n",
        "  \"\"\"\n",
        "  prompt = f\"\"\"\n",
        "  You are a strict speaking evaluator for the IELTS exam. Given the following information:\n",
        "  - Transcript: {transcript}\n",
        "  - Phonemes : (word, expected phoneme, observed phoneme) : {word_comparision_list}\n",
        "  give feedback on the pronounciation, Grammatical range and accuracy, and Lexical resource of the speaker, for each of the 3 cretiria output only a sentece of feedback, your output should be in the form of :\n",
        "  Fluency : Your fluency level is {fluency} with an avg GOP of {gop_score:.2f}\n",
        "  Pronounciation : very short feedback\n",
        "  Grammatical range and accuracy : feedback (do not mention anything about pronounciation here)\n",
        "  Lexical resource of the speaker : feedback (do not mention anything about pronounciation here)\n",
        "  Follow the IELTS speaking descriptors to give the speaker a very accurate feedback\n",
        "  Pronounciation rubic is {rubic[\"Pronunciation\"]}\n",
        "  Grammatical range and accuracy is {rubic[\"Grammatical range and accuracy\"]}\n",
        "  Lexical resource is {rubic[\"Lexical resource\"]}\n",
        "         \"\"\"\n",
        "  messages = [{\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt\n",
        "  }]\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True,\n",
        "      enable_thinking=False,\n",
        "  )\n",
        "  print(f\"\\n---\\nAnalyzing words:\")\n",
        "  _ = model.generate(\n",
        "      **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      temperature=0.7, top_p=0.8, top_k=20,\n",
        "      streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Tu-D5fIMBefy"
      },
      "outputs": [],
      "source": [
        "def evaluate_grammar_with_qwen(\n",
        "    user_text: str,\n",
        "    rubic: dict,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    memory: dict,\n",
        "    max_new_tokens: int = 64,\n",
        "):\n",
        "    \"\"\"\n",
        "    Analyze user_text for grammatical accuracy per IELTS rubric.\n",
        "    • If no errors → one sentence praising correctness.\n",
        "    • Else → JSON array of {location, issue, correction}.\n",
        "    \"\"\"\n",
        "    history = \"\\n\".join(f\"{role}: {txt}\" for role, txt in memory.get(\"history\", []))\n",
        "    prompt = f\"\"\"\n",
        "        You are **Qwen 3**, an IELTS *grammar‑only* evaluator.\n",
        "\n",
        "        Rubric (use **nothing else**):\n",
        "        {rubic[\"Grammatical range and accuracy\"]}\n",
        "\n",
        "        Conversation history:\n",
        "        {history if history else \"[none]\"}\n",
        "\n",
        "        RULES\n",
        "        ================================================================\n",
        "        1. Judge **only grammar** in *User text*.\n",
        "        2. Output **valid JSON** — nothing before or after it.\n",
        "\n",
        "          a) If **no errors**:\n",
        "              {{\n",
        "                \"result\": \"correct\",\n",
        "                \"message\": \"<two short, upbeat sentence (5‑10 words) praising the flawless grammar and encourge the student>\"\n",
        "              }}\n",
        "              • Invent a new praise line each run.\n",
        "              • Start with a capital letter, end with “!”\n",
        "              • No line breaks, no extra keys.\n",
        "\n",
        "          b) If **errors**:\n",
        "              {{\n",
        "                \"result\": \"incorrect\",\n",
        "                \"errors\": [\n",
        "                  {{\n",
        "                    \"location\": \"<exact excerpt>\",\n",
        "                    \"issue\": \"<brief description>\",\n",
        "                    \"correction\": \"<corrected text>\"\n",
        "                  }}\n",
        "                  {{ ...repeat per error... }}\n",
        "                ]\n",
        "              }}\n",
        "              • Keep the shown key order.\n",
        "              • No extra keys.\n",
        "\n",
        "        3. Produce **only** the JSON object.\n",
        "        ================================================================\n",
        "\n",
        "        User text:\n",
        "        \\\"\\\"\\\"{user_text}\\\"\\\"\\\"\n",
        "\n",
        "        Return JSON now.\n",
        "        \"\"\"\n",
        "\n",
        "    memory.setdefault(\"history\", []).append((\"Q\", prompt))\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 2)  Build chat prompt with Qwen template\n",
        "    # ------------------------------------------------------------\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 3)  Encode & generate (no streamer needed)\n",
        "    # ------------------------------------------------------------\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.3,\n",
        "        top_p=0.6,\n",
        "        top_k=10,\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 4)  Slice off the prompt tokens and decode\n",
        "    # ------------------------------------------------------------\n",
        "    reply_ids = output_ids[0, inputs[\"input_ids\"].size(1):]  # keep only the answer\n",
        "    feedback = tokenizer.decode(reply_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 5)  Store reply in memory & return\n",
        "    # ------------------------------------------------------------\n",
        "    memory[\"history\"].append((\"A\", feedback))\n",
        "    return feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "AzSqnUuSl9m8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948b8219-e63f-4d88-d0ae-9a0a83516aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LLM feedback ===\n",
            "{\n",
            "  \"result\": \"incorrect\",\n",
            "  \"errors\": [\n",
            "    {\n",
            "      \"location\": \"I do finished my homework today.\",\n",
            "      \"issue\": \"Subject-verb agreement error\",\n",
            "      \"correction\": \"I did finish my homework today.\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "memory = {}\n",
        "user_text = \"I do finished my homework today.\"\n",
        "\n",
        "from transformers import (\n",
        "    TextStreamer,\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4)  Run the evaluator\n",
        "# ------------------------------------------------------------\n",
        "feedback = evaluate_grammar_with_qwen(\n",
        "    user_text=user_text,\n",
        "    rubic=listeing_rubic,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    memory=memory,\n",
        "    max_new_tokens=128,\n",
        ")\n",
        "\n",
        "print(\"\\n=== LLM feedback ===\")\n",
        "print(feedback)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5eQ6vybu7W7"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "#  IELTS Speaking prompt builder + generator\n",
        "# ----------------------------------------------------------------------\n",
        "def generate_ielts_speaking_turn(\n",
        "    user_text: str,\n",
        "    part: int,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    memory: dict,\n",
        "    topic: Optional[str] = None,\n",
        "    max_new_tokens: int = 128,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate the next examiner turn for an IELTS Speaking interview.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    user_text   : Candidate's latest response ('' on first call).\n",
        "    part        : 1, 2, or 3  (raises ValueError otherwise).\n",
        "    model       : Hugging Face causal‑LM with chat weights (e.g. Qwen‑Chat).\n",
        "    tokenizer   : Matching tokenizer.\n",
        "    memory      : Dict with key \"history\"; retains full dialogue.\n",
        "    topic       : Explicit IELTS topic (str) or None → model picks one.\n",
        "    max_new_tokens : Generation length.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    examiner_turn : str   (the next question / cue card / follow‑up).\n",
        "    \"\"\"\n",
        "\n",
        "    # 0) Validate & sanitise inputs\n",
        "    if part not in (1, 2, 3):\n",
        "        raise ValueError(\"part must be 1, 2, or 3\")\n",
        "    topic_clause = (\n",
        "        f\"Use the fixed topic: **{topic}**.\"\n",
        "        if topic else\n",
        "        \"Select one IELTS‑appropriate topic yourself (e.g. *Hometown*, *Work*, \"\n",
        "        \"*Travel*, *Technology*, etc.) and clearly name it once.\"\n",
        "    )\n",
        "\n",
        "    # 1) Collect conversation so far\n",
        "    history_text = \"\\n\".join(f\"{role}: {txt}\" for role, txt in memory.get(\"history\", [])) \\\n",
        "                   or \"[none]\"\n",
        "\n",
        "    # 2) Build a part‑specific wrapper\n",
        "    # ------------------------------------------------------------------\n",
        "#  Part‑specific examiner instructions\n",
        "# ------------------------------------------------------------------\n",
        "    if part == 1:\n",
        "        part_instructions = (\n",
        "            \"PART 1 – Warm‑up (≈4 min, ≤12 Qs total)\\n\"\n",
        "            \"• At the start of *each* question include: \"\n",
        "            \"  'You have about 20 seconds to answer.'\\n\"\n",
        "            \"• Ask one short, familiar‑topic question per turn.\\n\"\n",
        "            \"• NEVER repeat a question already asked in this interview.\\n\"\n",
        "            \"• Output EXACTLY one line, prefixed with 'Examiner: '.\\n\"\n",
        "            \"  Example → Examiner: You have about 20 seconds to answer. Do you enjoy reading books?\\n\"\n",
        "        )\n",
        "    elif part == 2:\n",
        "        part_instructions = (\n",
        "            \"PART 2 – Long turn (≈3–4 min total)\\n\"\n",
        "            \"• Give one cue‑card:\\n\"\n",
        "            \"  ─ Topic title line (e.g. 'Describe a book you recently read').\\n\"\n",
        "            \"  ─ EXACTLY four bullet prompts starting with verbs (e.g. '• Where you got it').\\n\"\n",
        "            \"• End with: 'You have one minute to prepare and up to two minutes to speak.'\\n\"\n",
        "            \"• Output everything prefixed with 'Examiner: ' (single block).\\n\"\n",
        "            \"• Provide NO follow‑up until the candidate finishes.\\n\"\n",
        "        )\n",
        "    else:  # part == 3\n",
        "        part_instructions = (\n",
        "            \"PART 3 – Discussion (≈4–5 min, 4–6 Qs total)\\n\"\n",
        "            \"• Begin *each* question with: \"\n",
        "            \"  'You have about 30 to 40 seconds to answer.'\\n\"\n",
        "            \"• Ask one deeper, analytical question per turn.\\n\"\n",
        "            \"• Keep questions linked to the PART 2 topic and avoid repetition.\\n\"\n",
        "            \"• Output EXACTLY one line, prefixed with 'Examiner: '.\\n\"\n",
        "            \"  Example → Examiner: You have about 30 to 40 seconds to answer. \"\n",
        "            \"In what ways has technology changed the way we travel?\\n\"\n",
        "        )\n",
        "\n",
        "\n",
        "    # 3) Assemble the final system prompt\n",
        "    prompt = f\"\"\"\n",
        "          You are a certified IELTS Speaking examiner conducting PART {part} of the test.\n",
        "\n",
        "          {topic_clause}\n",
        "\n",
        "          Guidelines:\n",
        "          {part_instructions}\n",
        "          General rules for ALL parts:\n",
        "          • Maintain the formal IELTS style.\n",
        "          • NEVER answer for the candidate.\n",
        "          • Keep questions concise (max 25 words).\n",
        "          • Stop after producing your required output for this turn.\n",
        "\n",
        "          Conversation history:\n",
        "          {history_text}\n",
        "\n",
        "          Candidate's latest reply (if any):\n",
        "          \\\"\\\"\\\"{user_text}\\\"\\\"\\\"\n",
        "\n",
        "          Now generate ONLY the next examiner output as per the instructions.\n",
        "          \"\"\".strip()\n",
        "\n",
        "    # 4) Store prompt & call the model\n",
        "    memory.setdefault(\"history\", []).append((\"System\", prompt))\n",
        "    chat_messages = [{\"role\": \"system\", \"content\": prompt}]\n",
        "\n",
        "    encoded = tokenizer.apply_chat_template(\n",
        "        chat_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "    input_ids = tokenizer(encoded, return_tensors=\"pt\").to(model.device)\n",
        "    output = model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7, top_p=0.9, top_k=50,\n",
        "    )\n",
        "\n",
        "    # 5) Decode only the assistant’s new text\n",
        "    reply_ids = output[0, input_ids[\"input_ids\"].size(1):]\n",
        "    examiner_turn = tokenizer.decode(reply_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # 6) Update memory & return\n",
        "    memory[\"history\"].append((\"Examiner\", examiner_turn))\n",
        "    return examiner_turn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "T0L8mGmwwPgI"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "from pathlib import Path\n",
        "import json, re\n",
        "import torch  # for .device check on model\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 0)  Persistence helpers\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "_TOPIC_FILE = Path(\"ielts_recent_topics.json\")\n",
        "\n",
        "def _load_topic_cache() -> List[str]:\n",
        "    if _TOPIC_FILE.is_file():\n",
        "        try:\n",
        "            data = json.loads(_TOPIC_FILE.read_text())\n",
        "            if isinstance(data, list):\n",
        "                return [str(t) for t in data][:10]\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    return []\n",
        "\n",
        "def _save_topic_cache(topics: List[str]) -> None:\n",
        "    _TOPIC_FILE.write_text(json.dumps(topics[:10], ensure_ascii=False, indent=2))\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 1)  Main function\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "def generate_ielts_speaking_turn(\n",
        "    user_text: str,\n",
        "    part: int,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    memory: dict,\n",
        "    *,\n",
        "    topic: Optional[str] = None,\n",
        "    answer_seconds: Optional[float] = None,\n",
        "    max_new_tokens: int = 128,\n",
        "):\n",
        "    \"\"\"\n",
        "    Produce the next examiner line for an IELTS Speaking test, with\n",
        "    persistent topic‑diversity (last 10 topics stored on disk).\n",
        "    \"\"\"\n",
        "\n",
        "    # ── input checks ───────────────────────────────────────────\n",
        "    if part not in (1, 2, 3):\n",
        "        raise ValueError(\"part must be 1, 2, or 3\")\n",
        "    if answer_seconds is not None:\n",
        "        memory.setdefault(\"timings\", []).append(answer_seconds)\n",
        "\n",
        "    # ── history text ───────────────────────────────────────────\n",
        "    hist_txt = \"\\n\".join(f\"{r}: {t}\" for r, t in memory.get(\"history\", [])) or \"[none]\"\n",
        "\n",
        "    # ── part‑specific rules ────────────────────────────────────\n",
        "    if part == 1:\n",
        "        part_rules = (\n",
        "            \"PART 1 – Warm‑up (≈4 min, ≤12 Qs)\\n\"\n",
        "            \"• Prefix each question: 'You have about 20 seconds to answer.'\\n\"\n",
        "            \"• ONE short, familiar‑topic question; line starts 'Examiner: '.\\n\"\n",
        "            \"• Never write 'Candidate:'.\\n\"\n",
        "        )\n",
        "    elif part == 2:\n",
        "        part_rules = (\n",
        "            \"PART 2 – Long turn (≈3–4 min)\\n\"\n",
        "            \"• Output one cue‑card block in exactly 6 lines:\\n\"\n",
        "            \"  Examiner: (Topic: X)\\n\"\n",
        "            \"  Describe …\\n\"\n",
        "            \"  • …\\n\"\n",
        "            \"  • …\\n\"\n",
        "            \"  • …\\n\"\n",
        "            \"  • …\\n\"\n",
        "            \"  You have one minute to prepare and up to two minutes to speak.\\n\"\n",
        "            \"• No other lines, no 'Candidate:'.\\n\"\n",
        "        )\n",
        "    else:\n",
        "        part_rules = (\n",
        "            \"PART 3 – Discussion (≈4–5 min, 4–6 Qs)\\n\"\n",
        "            \"• Prefix each question: 'You have about 30 to 40 seconds to answer.'\\n\"\n",
        "            \"• ONE deeper, analytical question (line starts 'Examiner: ').\\n\"\n",
        "            \"• Never write 'Candidate:'.\\n\"\n",
        "        )\n",
        "\n",
        "    # ── initialise / sync topic cache ──────────────────────────\n",
        "    recent_topics = memory.setdefault(\"recent_topics\", _load_topic_cache())\n",
        "\n",
        "    def _remember(t: str):\n",
        "        if t and t not in recent_topics:\n",
        "            recent_topics.append(t)\n",
        "            if len(recent_topics) > 15:\n",
        "                recent_topics.pop(0)\n",
        "            _save_topic_cache(recent_topics)\n",
        "\n",
        "    # ── topic clause (avoid duplicates) ────────────────────────\n",
        "    if topic:\n",
        "        topic_clause = f\"Use the fixed topic **{topic}**.\"\n",
        "        _remember(topic)\n",
        "    else:\n",
        "        banned = \", \".join(recent_topics) if recent_topics else \"none\"\n",
        "        topic_clause = (\n",
        "            \"Choose ANY IELTS‑appropriate topic that is **not** in the following \"\n",
        "            f\"recent list (avoid: {banned}). \"\n",
        "            \"Announce it once at the beginning exactly like '(Topic: …)'.\"\n",
        "        )\n",
        "\n",
        "    # ── timing note ────────────────────────────────────────────\n",
        "    timing_note = (\n",
        "        f\"The candidate spent {round(answer_seconds)} seconds on the previous answer.\"\n",
        "        if answer_seconds is not None else\n",
        "        \"No timing data yet.\"\n",
        "    )\n",
        "\n",
        "    # ── build system prompt ───────────────────────────────────\n",
        "    sys_prompt = f\"\"\"\n",
        "You are a certified IELTS Speaking examiner running PART {part}.\n",
        "\n",
        "{topic_clause}\n",
        "\n",
        "{timing_note}\n",
        "\n",
        "=== Examiner Output Rules (STRICT) ===\n",
        "{part_rules}\n",
        "Global rules:\n",
        "• Use formal IELTS style (British spelling).\n",
        "• Never answer for the candidate.\n",
        "• Produce ONLY the examiner’s output for THIS turn.\n",
        "======================================\n",
        "\n",
        "Conversation so far:\n",
        "{hist_txt}\n",
        "\n",
        "Candidate's latest reply:\n",
        "\\\"\\\"\\\"{user_text}\\\"\\\"\\\"\n",
        "\n",
        "Generate the examiner’s next output now.\n",
        "\"\"\".strip()\n",
        "\n",
        "    # ── call model ────────────────────────────────────────────\n",
        "    memory.setdefault(\"history\", []).append((\"System\", sys_prompt))\n",
        "    encoded = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"system\", \"content\": sys_prompt}],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "    input_ids = tokenizer(encoded, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "    )\n",
        "    reply = tokenizer.decode(out[0, input_ids[\"input_ids\"].size(1):],\n",
        "                              skip_special_tokens=True).strip()\n",
        "\n",
        "    # ── store reply & auto‑capture topic ───────────────────────\n",
        "    memory[\"history\"].append((\"Examiner\", reply))\n",
        "    if not topic:\n",
        "        m = re.search(r\"\\(Topic:\\s*(.*?)\\)\", reply)\n",
        "        if m:\n",
        "            _remember(m.group(1).strip())\n",
        "\n",
        "    return reply\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-w94qnuwqQd",
        "outputId": "332e6dbe-ca9d-49cd-8279-e3516ac71f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: inputimeout in /usr/local/lib/python3.11/dist-packages (1.0.4)\n"
          ]
        }
      ],
      "source": [
        "pip install inputimeout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "1raJWJy7xtu6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "459e61e4-3403-4dc7-9b62-f66a08c2f09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Topic: A special gift you have received)  \n",
            "Describe a special gift you have received.  \n",
            "You should say:  \n",
            "- What the gift was  \n",
            "- Who gave it to you  \n",
            "- When you received it  \n",
            "- Why it was special to you  \n",
            "And explain why it still has meaning to you today.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-67-2400204765.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# ------------- no user input -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0muser_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Candidate: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[no answer]\"\u001b[0m    \u001b[0;31m# or any placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "TIME_LIMIT = 20          # seconds to “wait” per answer\n",
        "MAX_QS     = 12          # IELTS cap for Part 1\n",
        "memory     = {}\n",
        "part       = 2\n",
        "topic      = None\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 3)  Loop through Part 1 automatically\n",
        "# -----------------------------------------------------------------\n",
        "question_count  = 0\n",
        "prev_answer_sec = None\n",
        "candidate_a     = \"\"     # blank first “answer”\n",
        "\n",
        "while question_count < MAX_QS:\n",
        "    # Examiner’s question\n",
        "    examiner_q = generate_ielts_speaking_turn(\n",
        "        user_text=candidate_a,\n",
        "        part=part,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        memory=memory,\n",
        "        answer_seconds=prev_answer_sec,\n",
        "    )\n",
        "    question_count += 1\n",
        "    print(examiner_q)\n",
        "\n",
        "    # ------------- no user input -----------------\n",
        "    user_input=input(\"Candidate: \")\n",
        "    if user_input==None:\n",
        "        user_input = \"[no answer]\"    # or any placeholder\n",
        "    prev_answer_sec = TIME_LIMIT\n",
        "\n",
        "    memory[\"history\"].append((\"Candidate\", user_input))\n",
        "    # ---------------------------------------------\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 4)  Show entire Part‑1 transcript\n",
        "# -----------------------------------------------------------------\n",
        "print(\"\\n─── FULL PART 1 CONVERSATION ───\")\n",
        "for role, text in memory[\"history\"]:\n",
        "    if role in (\"Examiner\", \"Candidate\"):\n",
        "        print(f\"{role}: {text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORTptS4PGy7B"
      },
      "outputs": [],
      "source": [
        "def simulate_ielts_speaking_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    memory: dict,\n",
        "    part: int,\n",
        "    user_response: str | None = None,\n",
        "    topic: str | None = None,\n",
        "    max_new_tokens: int = 128,\n",
        "):\n",
        "    \"\"\"\n",
        "    Return the next examiner turn as pure text (nothing prints to console).\n",
        "    \"\"\"\n",
        "    if user_response:\n",
        "        memory.setdefault(\"history\", []).append((\"Candidate\", user_response))\n",
        "\n",
        "    # ---------------- Part‑specific scaffolding ----------------\n",
        "    if part == 1:\n",
        "        timing_line = \"You have a maximum of 5 minutes for this section.\"\n",
        "        task_lines = [\n",
        "            \"1. Greet the candidate and verify their identity.\",\n",
        "            \"2. Ask TWO open‑ended questions on familiar topics \"\n",
        "            \"(e.g. hometown, work / study, free‑time).\",\n",
        "        ]\n",
        "        if topic:\n",
        "            task_lines.append(\n",
        "                f\"   • The questions MUST relate to the provided topic: '{topic}'.\"\n",
        "            )\n",
        "\n",
        "    elif part == 2:\n",
        "        timing_line = (\n",
        "            \"You will give the candidate 1 minute to prepare and up to 2 minutes to speak.\"\n",
        "        )\n",
        "        task_lines = [\n",
        "            \"1. Present ONE cue‑card with:\",\n",
        "            \"     • a main topic title,\",\n",
        "            \"     • exactly THREE bullet‑point prompts.\",\n",
        "            \"2. After 1 minute of note‑taking, say exactly: \"\n",
        "            \"'Please begin speaking now.'\",\n",
        "            \"3. If the candidate stops before 1 minute, pause silently for 5 seconds \"\n",
        "            \"then ask one rounding‑off question.\",\n",
        "        ]\n",
        "        if topic:\n",
        "            task_lines.insert(\n",
        "                0, f\"The cue card MUST focus on the given topic: '{topic}'.\"\n",
        "            )\n",
        "        else:\n",
        "            task_lines.insert(\n",
        "                0,\n",
        "                \"Choose a concrete, everyday topic suitable for IELTS Band 5–8 \"\n",
        "                \"(e.g. 'Describe a memorable journey').\",\n",
        "            )\n",
        "\n",
        "    elif part == 3:\n",
        "        timing_line = \"You have up to 5 minutes for this section.\"\n",
        "        task_lines = [\n",
        "            \"1. Ask TWO broad, abstract questions linked to the Part 2 theme.\",\n",
        "            \"2. Encourage extended answers with at least one follow‑up prompt each.\",\n",
        "            \"3. Paraphrase candidate responses only when necessary for clarity.\",\n",
        "        ]\n",
        "        if topic:\n",
        "            task_lines.insert(\n",
        "                0,\n",
        "                f\"All questions MUST expand on the theme: '{topic}'. \"\n",
        "                \"Focus on causes, consequences, or comparisons.\",\n",
        "            )\n",
        "        else:\n",
        "            task_lines.insert(\n",
        "                0,\n",
        "                \"Base your questions on whatever topic you set in Part 2 \"\n",
        "                \"or choose a logical expansion if none was set explicitly.\",\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"part must be 1, 2, or 3\")\n",
        "\n",
        "    # Build prompt blocks\n",
        "    history_block = \"\\n\".join(f\"{r}: {t}\" for r, t in memory.get(\"history\", [])) or \"[no prior turns]\"\n",
        "    tasks_block = \"\\n\".join(task_lines)\n",
        "\n",
        "    prompt = (\n",
        "        \"SYSTEM:\\n\"\n",
        "        \"You are the OFFICIAL IELTS Speaking examiner. Follow Cambridge assessment \"\n",
        "        \"standards precisely. Maintain examiner neutrality (no personal opinions).\\n\\n\"\n",
        "        \"IELTS PROCEDURE FOR THIS PART\\n\"\n",
        "        f\"{timing_line}\\n{tasks_block}\\n\\n\"\n",
        "        \"CONVERSATION SO FAR\\n\"\n",
        "        f\"{history_block}\\n\\n\"\n",
        "        \"YOUR TASK\\n\"\n",
        "        \"Produce ONLY the next examiner utterance, formatted as plain text ready to be \"\n",
        "        \"read aloud. Do NOT output more than one examiner turn.\"\n",
        "    )\n",
        "\n",
        "    # Save prompt in history and generate\n",
        "    memory.setdefault(\"history\", []).append((\"Q\", prompt))\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=20,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Get only the newly‑generated tokens\n",
        "    generated = output_ids[0, input_ids[\"input_ids\"].shape[-1]:]\n",
        "    examiner_utterance = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
        "\n",
        "    memory[\"history\"].append((\"Examiner\", examiner_utterance))\n",
        "    return examiner_utterance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_AfRHgPGHx8",
        "outputId": "1dfc3e2f-d0b3-412d-b445-317791e46cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examiner: What do you usually do in your free time?\n",
            "\n",
            "Candidate: next\n",
            "\n",
            "Examiner: Describe a place you would like to visit in the future.  \n",
            "- Where is this place?  \n",
            "- Why would you like to visit it?  \n",
            "- What would you do there?  \n",
            "Please begin speaking now.  \n",
            "What makes this place special to you?  \n",
            "Have you ever visited a similar place before?\n",
            "\n",
            "Session terminated.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"IELTS Speaking simulation helper – all LLM‑driven\n",
        "----------------------------------------------------------\n",
        "Utilities\n",
        "---------\n",
        "1. _auto_candidate_reply():  If the candidate hits Enter, Qwen fabricates a brief, natural answer.\n",
        "2. simulate_ielts_speaking_text():  Produces the next examiner turn for Parts 1‑3 – every question\n",
        "   or cue card is generated by the LLM (no hard‑coded questions).\n",
        "\n",
        "Conversation state\n",
        "------------------\n",
        "`memory` is a dict with:\n",
        "    memory[\"history\"] : list[tuple(role, text)]  – running transcript\n",
        "    memory[\"p1_q_count\"] : int                    – how many Part‑1 questions already asked\n",
        "Only the **last 4** turns are given to the model each call.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 0) Helper: fabricate an answer if the candidate skips a reply\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "def _auto_candidate_reply(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    last_question: str,\n",
        "    max_new_tokens: int = 48,\n",
        ") -> str:\n",
        "    \"\"\"Return a short (≤2‑sentence) answer generated by Qwen.\"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are an IELTS Speaking candidate. Answer the examiner's question naturally, \"\n",
        "                \"in one or two sentences.\"\n",
        "            ),\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": f\"Examiner asked: '{last_question}'  Your answer:\"},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.9,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(out[0, inputs.input_ids.shape[-1] :], skip_special_tokens=True).strip()\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 1)  Main: produce the next examiner turn (all parts LLM‑driven)\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "def simulate_ielts_speaking_text(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    memory: Dict[str, List[Tuple[str, str]]],\n",
        "    part: int,\n",
        "    user_response: Optional[str] = None,\n",
        "    topic: Optional[str] = None,\n",
        "    max_new_tokens: int = 120,\n",
        ") -> str:\n",
        "    \"\"\"Return ONLY the next examiner utterance (plain text ≤40 words, ends with '?').\"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # A. Save candidate reply (auto‑generate if blank)\n",
        "    # ------------------------------------------------------------------\n",
        "    if user_response is not None:\n",
        "        if user_response.strip() == \"\":\n",
        "            last_q = next((txt for role, txt in reversed(memory.get(\"history\", [])) if role == \"Examiner\"), \"\")\n",
        "            auto_ans = _auto_candidate_reply(model, tokenizer, last_q)\n",
        "            memory.setdefault(\"history\", []).append((\"Candidate\", auto_ans))\n",
        "        else:\n",
        "            memory.setdefault(\"history\", []).append((\"Candidate\", user_response))\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # B. Part‑specific instruction blocks\n",
        "    # ------------------------------------------------------------------\n",
        "    p1_q_count = memory.get(\"p1_q_count\", 0)\n",
        "\n",
        "    if part == 1:\n",
        "        timing_line = \"Part 1 lasts 4 ‑ 5 minutes.\"\n",
        "        if p1_q_count < 2:\n",
        "            task_lines = [\n",
        "                \"Ask ONE open‑ended question on a NEW familiar topic (hometown, work/study, free‑time, family, etc.).\",\n",
        "                \"Do not repeat any previous Part 1 question.\",\n",
        "            ]\n",
        "        else:\n",
        "            task_lines = [\n",
        "                \"Politely conclude Part 1 and instruct the candidate to say 'next' when ready for Part 2.\"\n",
        "            ]\n",
        "    elif part == 2:\n",
        "        timing_line = (\n",
        "            \"Give the candidate 1 minute to prepare, then up to 2 minutes to speak. \"\n",
        "            \"The cue‑card must include EXACTLY three bullet prompts. After 60 seconds say: 'Please begin speaking now.'\"\n",
        "        )\n",
        "        task_lines = []\n",
        "        if topic:\n",
        "            task_lines.append(f\"The cue card topic MUST be: '{topic}'.\")\n",
        "        else:\n",
        "            task_lines.append(\"Choose an everyday topic suitable for IELTS Band 5‑8.\")\n",
        "        task_lines.append(\"End with two brief follow‑up questions.\")\n",
        "    elif part == 3:\n",
        "        timing_line = \"Part 3 lasts about 5 minutes.\"\n",
        "        task_lines = [\n",
        "            \"Ask TWO abstract, opinion‑based questions linked to the Part 2 topic, each followed by ONE probing follow‑up.\",\n",
        "        ]\n",
        "        if topic:\n",
        "            task_lines.insert(0, f\"Base all questions on this theme: '{topic}'.\")\n",
        "    else:\n",
        "        raise ValueError(\"part must be 1, 2, or 3\")\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # C. Recent conversation context (last 4 turns only)\n",
        "    # ------------------------------------------------------------------\n",
        "    MAX_HISTORY = 4\n",
        "    recent = memory.get(\"history\", [])[-MAX_HISTORY:]\n",
        "    history_block = \"\\n\".join(f\"{r}: {t}\" for r, t in recent) or \"[no recent turns]\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # D. Compose prompts for Qwen\n",
        "    # ------------------------------------------------------------------\n",
        "    system_prompt = (\n",
        "        \"You are Qwen‑3, acting as the OFFICIAL IELTS Speaking examiner.\\n\"\n",
        "        \"Follow Cambridge procedure strictly. Remain neutral; never explain your reasoning.\\n\"\n",
        "        \"Here is hidden conversation context for reference – DO NOT reveal or quote it:\\n\"\n",
        "        f\"{history_block}\\n\"\n",
        "        \"Output ONLY the next examiner utterance (≤40 words, end with a question mark unless concluding the part).\"\n",
        "    )\n",
        "\n",
        "    user_prompt = \"\\n\".join([\n",
        "        f\"IELTS SPEAKING PART {part} – TASK\",\n",
        "        timing_line,\n",
        "        *task_lines,\n",
        "        \"Make sure your visible output follows the rules above.\"\n",
        "    ])\n",
        "\n",
        "    full_prompt = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # E. Generate examiner turn\n",
        "    # ------------------------------------------------------------------\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    examiner = tokenizer.decode(out[0, inputs.input_ids.shape[-1] :], skip_special_tokens=True).strip()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # F. Update memory & counters\n",
        "    # ------------------------------------------------------------------\n",
        "    memory.setdefault(\"history\", []).append((\"Examiner\", examiner))\n",
        "    if part == 1 and p1_q_count < 2:\n",
        "        memory[\"p1_q_count\"] = p1_q_count + 1\n",
        "\n",
        "    return examiner\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Example interactive loop (comment out on import)\n",
        "# -----------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    mem: Dict[str, List[Tuple[str, str]]] = {}\n",
        "    part = 1\n",
        "    print(\"Examiner:\", simulate_ielts_speaking_text(model, tokenizer, mem, part))\n",
        "\n",
        "    try:\n",
        "        while part <= 3:\n",
        "            answer = input(\"\\nCandidate: \")\n",
        "            if answer.lower().strip() == \"next\":\n",
        "                part += 1\n",
        "                if part > 3:\n",
        "                    break\n",
        "                print(\"\\nExaminer:\", simulate_ielts_speaking_text(model, tokenizer, mem, part))\n",
        "            else:\n",
        "                print(\n",
        "                    \"\\nExaminer:\",\n",
        "                    simulate_ielts_speaking_text(\n",
        "                        model, tokenizer, mem, part, user_response=answer\n",
        "                    ),\n",
        "                )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSession terminated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fepz1pjmQSm0"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken 30NkU4x5BjWRCW1VsFQ5zNw0h8q_4YtQ6sjZfds535NfAoyAV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZlYjuURVeNX"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -------------------------------------------------------------------\n",
        "# FastAPI + ngrok  • Pronunciation‑AI server  (prints → feedback fixed)\n",
        "# -------------------------------------------------------------------\n",
        "import io, contextlib, uuid, zipfile, difflib, re\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import librosa, numpy as np, soundfile as sf, torch\n",
        "from transformers import pipeline\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from fastapi.responses import JSONResponse, FileResponse\n",
        "from pyngrok import ngrok\n",
        "import uvicorn, nest_asyncio\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ---------------- configuration ------------------------------------\n",
        "PRE_MS, POST_MS = 2000, 1000          # context around each clip\n",
        "TMP_DIR = Path(\"sessions\"); TMP_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# ---------------- Whisper pipeline ---------------------------------\n",
        "ASR = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-small.en\",\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    chunk_length_s=30,\n",
        "    return_timestamps=\"word\",\n",
        "    ignore_warning=True,\n",
        ")\n",
        "\n",
        "def transcribe_with_whisper(wav: Path, fallback_ms=300):\n",
        "    out = ASR(str(wav), batch_size=8)\n",
        "    transcript = out[\"text\"].strip()\n",
        "    chunks, prev_end = [], 0\n",
        "    for c in out[\"chunks\"]:\n",
        "        w, (ts, te) = c[\"text\"].strip().lower(), c[\"timestamp\"]\n",
        "        if ts is None: ts = prev_end / 1000\n",
        "        if te is None: te = ts + fallback_ms / 1000\n",
        "        s_ms, e_ms = int(ts*1000), int(te*1000)\n",
        "        prev_end = e_ms\n",
        "        chunks.append((w, s_ms, e_ms))\n",
        "    return transcript, chunks\n",
        "\n",
        "# ---------------- word‑matching helpers ----------------------------\n",
        "def fuzzy_eq(a, b, cut=0.83):  # Levenshtein ratio\n",
        "    return difflib.SequenceMatcher(None, a, b).ratio() >= cut\n",
        "\n",
        "def locate_word(tgt: str, chunks: List, idx: int):\n",
        "    for i in range(idx, len(chunks)):\n",
        "        w, s, e = chunks[i]\n",
        "        if w == tgt or fuzzy_eq(w, tgt):\n",
        "            return i, s, e\n",
        "    for i in range(idx, len(chunks)-1):\n",
        "        w1, s1, _  = chunks[i]\n",
        "        w2, _,  e2 = chunks[i+1]\n",
        "        if w1 + w2 == tgt or fuzzy_eq(w1 + w2, tgt):\n",
        "            return i+1, s1, e2\n",
        "    return None, None, None\n",
        "\n",
        "# ---------------- clip + zip helper --------------------------------\n",
        "def save_error_clips(wav: Path, wrongs, sdir: Path):\n",
        "    y, sr = librosa.load(wav, sr=None, mono=True)\n",
        "    total_ms = int(len(y)/sr*1000)\n",
        "    cdir = sdir / \"error_clips\"; cdir.mkdir(exist_ok=True)\n",
        "\n",
        "    segs, meta = [], []\n",
        "    for i, (w, _, _, ws, we) in enumerate(wrongs, 1):\n",
        "        cs, ce = max(0, ws-PRE_MS), min(total_ms, we+POST_MS)\n",
        "        seg = y[int(cs/1000*sr): int(ce/1000*sr)]\n",
        "        segs.append(seg)\n",
        "        sf.write(cdir / f\"{i:02d}_{w}.wav\", seg, sr)\n",
        "        meta.append((w, cs, ce))\n",
        "\n",
        "    err_wav = sdir / \"errors.wav\"\n",
        "    if segs:\n",
        "        sf.write(err_wav, np.concatenate(segs), sr)\n",
        "\n",
        "    zpath = sdir / \"pronunciation_errors.zip\"\n",
        "    with zipfile.ZipFile(zpath, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "        for f in cdir.rglob(\"*\"):\n",
        "            zf.write(f, arcname=f.relative_to(sdir))\n",
        "        if err_wav.exists():\n",
        "            zf.write(err_wav, arcname=err_wav.name)\n",
        "    return meta, zpath\n",
        "\n",
        "# ---------------- FastAPI app --------------------------------------\n",
        "app = FastAPI(title=\"Pronunciation‑AI\")\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze(audio: UploadFile = File(...)):\n",
        "    if not audio.filename.lower().endswith(\".wav\"):\n",
        "        raise HTTPException(400, \"Only .wav accepted\")\n",
        "\n",
        "    sid = uuid.uuid4().hex\n",
        "    sdir = TMP_DIR / sid; sdir.mkdir()\n",
        "    wav = sdir / \"input.wav\"; wav.write_bytes(await audio.read())\n",
        "\n",
        "    # ---------- pipeline ----------\n",
        "    _ = audio_to_phoneme(wav, return_logits=True)\n",
        "    pred_ph = audio_to_phoneme(wav, return_logits=False)\n",
        "    gop, flu_num = audio_to_phoneme_gop(wav)\n",
        "    transcript, chunks = transcribe_with_whisper(wav)\n",
        "\n",
        "    lexicon, ref_words = generate_reference_phoneme(transcript)\n",
        "    expected = \" \".join(p for _, p in lexicon)\n",
        "    al_ref, al_rec = align_sequences(expected, pred_ph, transcript)\n",
        "    ref_start = find_word_start_positions(\"\".join(al_ref))\n",
        "    rec_split = [re.sub(r\"[\\s_]+$\", \"\", w)\n",
        "                 for w in split_recorded_sequence(\"\".join(al_rec), ref_start)]\n",
        "    ref_split = [re.sub(r\"[\\s_]+$\", \"\", w)\n",
        "                 for w in split_recorded_sequence(\"\".join(al_ref), ref_start)]\n",
        "\n",
        "    comp = list(zip(ref_words, ref_split, rec_split))\n",
        "    wrongs, idx = [], 0\n",
        "    for w, rp, rc in comp:\n",
        "        if rp == rc: continue\n",
        "        pos, s_ms, e_ms = locate_word(w.lower(), chunks, idx)\n",
        "        if pos is not None:\n",
        "            wrongs.append((w, rp, rc, s_ms, e_ms))\n",
        "            idx = pos + 1\n",
        "\n",
        "    clip_meta, zpath = save_error_clips(wav, wrongs, sdir)\n",
        "\n",
        "    flu_label = (\"High fluency\" if gop > 0.9 else\n",
        "                 \"Moderate fluency\" if gop > 0.75 else\n",
        "                 \"Low fluency\")\n",
        "\n",
        "    # ---------- capture Qwen output ----------\n",
        "    try:\n",
        "        buf = io.StringIO()\n",
        "        with contextlib.redirect_stdout(buf):\n",
        "            out = analyze_pronunciation_with_qwen(\n",
        "                transcript, comp, irubic,\n",
        "                model, flu_num, gop, tokenizer,\n",
        "                max_new_tokens=150,\n",
        "            )\n",
        "        feedback = (out if out else buf.getvalue()).strip()\n",
        "        if not feedback:\n",
        "            feedback = \"⚠️  Generation returned nothing.\"\n",
        "    except Exception as err:\n",
        "        print(\"!! Qwen failed:\", err)\n",
        "        feedback = \"⚠️  Could not generate feedback due to an internal error.\"\n",
        "\n",
        "    mis_words = [\n",
        "        {\"word\": w, \"ref\": rp, \"rec\": rc,\n",
        "         \"clip_start_ms\": cs, \"clip_end_ms\": ce}\n",
        "        for (w, rp, rc, _, _), (_, cs, ce) in zip(wrongs, clip_meta)\n",
        "    ]\n",
        "\n",
        "    return JSONResponse({\n",
        "        \"session_id\": sid,\n",
        "        \"transcript\": transcript,\n",
        "        \"gop_score\": float(gop),\n",
        "        \"fluency\": flu_label,\n",
        "        \"feedback\": feedback,\n",
        "        \"mispronounced\": mis_words,\n",
        "        \"zip_url\": f\"/download/{sid}\"\n",
        "    })\n",
        "\n",
        "@app.get(\"/download/{sid}\")\n",
        "async def download_zip(sid: str):\n",
        "    z = TMP_DIR / sid / \"pronunciation_errors.zip\"\n",
        "    if not z.exists():\n",
        "        raise HTTPException(404, \"Not found\")\n",
        "    return FileResponse(z, media_type=\"application/zip\",\n",
        "                        filename=\"pronunciation_errors.zip\")\n",
        "\n",
        "# ---------------- run + ngrok ------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    public = ngrok.connect(8000).public_url\n",
        "    print(f\"🚀 Public endpoint: {public}/analyze\")\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7813603,
          "sourceId": 12416783,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c09c22fb7827415986188defddce5823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae6ecdae20ab48988f143c3f63677208",
              "IPY_MODEL_d31f0df406934d48ba6f00646c98bb23",
              "IPY_MODEL_5d97169e778345808ae59469569ec469"
            ],
            "layout": "IPY_MODEL_66ea7a99b2354201ab3d2c91398ca099"
          }
        },
        "ae6ecdae20ab48988f143c3f63677208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd394e3a49e04fd4a815b8390fffaea9",
            "placeholder": "​",
            "style": "IPY_MODEL_cbf43c15d57f43bc8c245d46edac213d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d31f0df406934d48ba6f00646c98bb23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cad52837edb48ca9307191d0325b6ea",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e991be4fa83e4e3397c080553c55e408",
            "value": 3
          }
        },
        "5d97169e778345808ae59469569ec469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6469e76898ef4257a75e007db2399bbc",
            "placeholder": "​",
            "style": "IPY_MODEL_d7af522e0cb34a6d953d96499a09b32d",
            "value": " 3/3 [00:03&lt;00:00,  1.01s/it]"
          }
        },
        "66ea7a99b2354201ab3d2c91398ca099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd394e3a49e04fd4a815b8390fffaea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf43c15d57f43bc8c245d46edac213d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cad52837edb48ca9307191d0325b6ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e991be4fa83e4e3397c080553c55e408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6469e76898ef4257a75e007db2399bbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7af522e0cb34a6d953d96499a09b32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}