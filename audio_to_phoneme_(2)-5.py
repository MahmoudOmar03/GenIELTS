# -*- coding: utf-8 -*-
"""audio-to-phoneme (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c94QAwIYzC1CvmswTp6fAUmWl74Zi8Fh

# DEPENDENCIES
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install phonemizer
!pip install python-Levenshtein
!pip install requests ipywidgets sounddevice soundfile
!pip install sequence_align

!sudo apt-get install espeak-ng
# MIGHT ALSO NEED TO INSTALL FFMPEG

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

!pip install -U bitsandbytes

!pip install fastapi uvicorn pyngrok TTS pydub requests --quiet

"""# IELTS SPEAKING RUBIC FOR CONTEXT AND INFO GROUNDING TO THE LLLM

# audio file
"""

audio = "/content/drive/MyDrive/KAUST/Summer/Project/Recording.wav"

!pip uninstall numpy -y

!pip install numpy==1.26.4

"""# Configrution"""

listeing_rubic = {
    "Grammatical range and accuracy": (
        "Band‑9 (39‑40/40 correct): Fully understands gist, detail, inference, attitude and purpose in any accent or speed. "
        "Band‑8 (35‑38): Very good comprehension with only rare slips on nuance or low‑frequency lexis. "
        "Band‑7 (30‑34): Good grasp of main ideas and most details; occasional difficulty with dense or rapid speech. "
        "Band‑6 (23‑29): Generally effective in familiar contexts but misses some detail, inference and fast passages. "
        "Band‑5 (16‑22): Modest comprehension of overall meaning in predictable situations; frequent loss of detail. "
        "Band‑4 (11‑15): Limited comprehension; can catch basic factual info in slow, clear speech only. "
        "Band‑3 (6‑10): Very limited comprehension; recognises isolated words and familiar phrases. "
        "Band‑2 (3‑5): Intermittent comprehension; picks up only a few isolated words despite repetition. "
        "Band‑1 (0‑2): No functional ability to understand spoken English."
    )
}

irubic = {"Pronunciation":
              ["Band 1 : Can produce occasional individual words and phonemes that are recognisable, but no overall meaning is conveyed. Unintelligible.","Band 2 : Uses few acceptable phonological features (possibly because sample is insufficient). Overall problems with delivery impair attempts at connected speech. Individual words and phonemes are mainly mispronounced and little meaning is conveyed. Often unintelligible."
    ,"Band 3 : Displays some features of band 2, and some, but not all, of the positive features of band 4.", "Band 4 : Uses some acceptable phonological features, but the range is limited. Produces some acceptable chunking, but there are frequent lapses in overall rhythm. Attempts to use intonation and stress, but control is limited. Individual words or phonemes are frequently mispronounced, causing lack of clarity. Understanding requires some effort and there may be patches of speech that cannot be understood.",
    "Band 5 : Displays all the positive features of band 4, and some, but not all, of the positive features of band 6.", "Band 6 : Uses a range of phonological features, but control is variable. Chunking is generally appropriate, but rhythm may be affected by a lack of stress-timing and/or a rapid speech rate. Some effective use of intonation and stress, but this is not sustained. Individual words or phonemes may be mispronounced but this causes only occasional lack of clarity. Can generally be understood throughout without much effort.",
    "Band 7 : Displays all the positive features of band 6, and some, but not all, of the positive features of band 8.", "Band 8 : Uses a wide range of phonological features to convey precise and/or subtle meaning. Can sustain appropriate rhythm. Flexible use of stress and intonation across long utterances, despite occasional lapses. Can be easily understood throughout. Accent has minimal effect on intelligibility.", "Band 9 : Uses a full range of phonological features to convey precise and/or subtle meaning. Flexible use of features of connected speech is sustained throughout. Can be effortlessly understood throughout. Accent has no effect on intelligibility."
        ], "Grammatical range and accuracy" : [
        "Band 1 : No rateable languageunless memorised.",
        "Band 2 : No evidence of basic sentence forms.",
        "Band 3 : Basic sentence forms are attempted but grammatical errors are numerous except in apparently memorised utterances.",
        "Band 4 : Can produce basic sentence forms and some short utterances are error-free. Subordinate clauses are rare and, overall, turns are short, structures are repetitive and errors are frequent.",
        "Band 5 : Basic sentence forms are fairly well controlled for accuracy. Complex structures are attempted but these are limited in range, nearly always contain errors and may lead to the need for reformulation.",
        "Band 6 : Produces a mix of short and complex sentence forms and a variety of structures with limited flexibility. Though errors frequently occur in complex structures, these rarely impede communication.",
        "Band 7 : A range of structures flexibly used. Error-free sentences are frequent. Both simple and complex sentences are used effectively despite some errors. A few basic errors persist.",
        "Band 8 : Wide resource, readily and flexibly used to discuss all topics and convey precise meaning. Skilful use of less common and idiomatic items despite occasional inaccuracies in word choice and collocation. Effective use of paraphrase as required.",
        "Band 9 : Structures are precise and accurate at all times, apart from ‘mistakes’ characteristic of native speaker speech."
        ], "Lexical resource" : [
        "Band 1 : No resource bar a few isolated words. No communication possible.",
        "Band 2 : Very limited resource. Utterances consist of isolated words or memorised utterances. Little communication possible without the support of mime or gesture.",
        "Band 3 : Resource limited to simple vocabulary used primarily to convey personal information. Vocabulary inadequate for unfamiliar topics.",
        "Band 4 : Resource sufficient for familiar topics but only basic meaning can be conveyed on unfamiliar topics. Frequent inappropriaciesand errors in word choice. Rarely attempts paraphrase.",
        "Band 5 : Resource sufficient to discuss familiar and unfamiliar topics but there is limited flexibility. Attempts paraphrase but not always with success.",
        "Band 6 : Resource sufficient to discuss topics at length. Vocabulary use may be inappropriate but meaning is clear. Generally able to paraphrase successfully.",
        "Band 7 : Resource flexibly used to discuss a variety of topics. Some ability to use less common and idiomatic items and an awareness of style and collocation is evident though inappropriaciesoccur. Effective use of paraphrase as required.",
        "Band 8 : Wide resource, readily and flexibly used to discuss all topics and convey precise meaning. Skilful use of less common and idiomatic items despite occasional inaccuracies in word choice and collocation. Effective use of paraphrase as required.",
        "Band 9 : Total flexibility and precise use in all contexts. Sustained use of accurate and idiomatic language."
        ]}

"""# Extracting actual phonemes from the audio (in : audio, out : phoneme seq)"""

import re
from transformers import TextStreamer

from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch
import librosa
import soundfile as sf
import numpy as np

def audio_to_phoneme(audio_file_path,return_logits=False):
  # Choose a model checkpoint
  model_name = "facebook/wav2vec2-lv-60-espeak-cv-ft" # Example: multilingual phoneme model

  # Load the processor and model
  processorp = Wav2Vec2Processor.from_pretrained(model_name)
  modelp = Wav2Vec2ForCTC.from_pretrained(model_name)

  # Load audio file
  speech, sample_rate = librosa.load(audio_file_path, sr=None)


  # Resample to 16kHz if needed
  if sample_rate != 16000:
      speech = librosa.resample(speech, orig_sr=sample_rate, target_sr=16000)

  # Prepare input values for the model
  # The `return_tensors="pt"` argument ensures PyTorch tensors are returned.
  input_values = processorp(speech, sampling_rate=16000, return_tensors="pt").input_values
  device = "cuda" if torch.cuda.is_available() else "cpu"
  modelp.to(device)
  input_values = input_values.to(device)
  with torch.no_grad():
      logits = modelp(input_values).logits
      if return_logits:
        return logits

  # Take argmax to get the most probable phoneme IDs
  predicted_ids = torch.argmax(logits, dim=-1)

  # Decode the predicted IDs to phoneme string
  phoneme_transcription = processorp.batch_decode(predicted_ids)

  print(f"Phoneme Transcription: {phoneme_transcription}")
  phoneme = " ".join(phoneme_transcription)
  return phoneme

"""# calculatin the avg Goodness of pronounciation of the entire audio (in : uses the logits of the model, out: GOP score + fluency level)"""

import torch.nn.functional as F


def audio_to_phoneme_gop(audio_file_path):
  logits = audio_to_phoneme(audio_file_path,return_logits=True)

  # Get probabilities
  probs = F.softmax(logits, dim=-1)  # shape: (batch, time, vocab)

  # Get predicted phoneme IDs
  predicted_ids = torch.argmax(probs, dim=-1)

  # For each time step, get the probability of the predicted phoneme
  batch_idx = 0  # if batch size is 1
  phoneme_probs = probs[batch_idx, torch.arange(probs.shape[1]), predicted_ids[batch_idx]]

  # Average probability as a simple fluency/confidence score
  gop_score = phoneme_probs.mean().item()
  print(f"GOP Score (average confidence): {gop_score:.3f}")


  def fluency_level(gop_score):
      if gop_score > 0.9:
          return "High fluency"
      elif gop_score > 0.75:
          return "Moderate fluency"
      else:
          return "Low fluency"

  fluency = fluency_level(gop_score)
  return gop_score, fluency

"""# loads whisper small to extract transcript"""

import torchaudio
from transformers import AutoTokenizer, AutoModelForSpeechSeq2Seq, AutoProcessor, BitsAndBytesConfig
from tqdm import tqdm

model_id = "openai/whisper-small.en"
device="cuda" if torch.cuda.is_available() else "cpu"
# Load processor
processor = AutoProcessor.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)


model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)
model.to(device)

"""# extracts transcript of the audio (note: fallback not implemented) (in : audio, out : transcript)"""

# Alternative approach using Transformers pipeline (more robust)
from transformers import pipeline

def audio_to_text_pipeline(audio_file_path, chunk_length_s=30):
    """
    Convert audio file to text using Transformers pipeline (most reliable method)

    Args:
        audio_file_path (str): Path to the audio file
        chunk_length_s (int): Length of audio chunks in seconds

    Returns:
        str: Transcribed text
    """
    try:
        # Create a speech recognition pipeline
        pipe = pipeline(
            "automatic-speech-recognition",
            model=model_id,
            device=0 if torch.cuda.is_available() else -1,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            chunk_length_s=chunk_length_s,
            return_timestamps=True  # Enable timestamps for better alignment
        )

        # Process the audio file
        result = pipe(audio_file_path)

        # Extract text from result
        if isinstance(result, dict):
            return result.get("text", "")
        elif isinstance(result, list):
            return " ".join([chunk.get("text", "") for chunk in result])
        else:
            return str(result)

    except Exception as e:
        print(f"Pipeline method failed: {e}")
        print("Falling back to manual method...")
        return audio_to_text_chunked(audio_file_path, chunk_length_s=chunk_length_s)

def audio_to_text_simple(audio_file_path):
    """
    Simple audio to text conversion using librosa and basic processing

    Args:
        audio_file_path (str): Path to the audio file

    Returns:
        str: Transcribed text
    """
    try:
        # Try pipeline method first (most reliable)
        return audio_to_text_pipeline(audio_file_path)
    except Exception as e:
        print(f"Pipeline failed, trying chunked method: {e}")
        try:
            # Fallback to chunked method
            return audio_to_text_chunked(audio_file_path)
        except Exception as e2:
            print(f"Chunked method failed: {e2}")
            # Final fallback
            return audio_to_text(audio_file_path)

'''transcript = audio_to_text_simple(audio)
print(transcript)'''

"""# extract expected phonemes (in : transcript, out : expected phonemes)"""

import phonemizer
from phonemizer.punctuation import Punctuation
from phonemizer.backend import EspeakBackend
from phonemizer.separator import Separator

def generate_reference_phoneme(reference_text):
    text = Punctuation(';:,.!"?()').remove(reference_text)
    ref_words = [w.lower() for w in text.strip().split(' ') if w]


    # initialize the espeak backend for English
    backend = EspeakBackend('en-us')

    # separate phones by a space and ignoring words boundaries
    separator = Separator(phone='', word=None)

    # build the lexicon by phonemizing each word one by one. The backend.phonemize
    # function expect a list as input and outputs a list.
    lexicon = [ (word, backend.phonemize([word], separator=separator, strip=True)[0])
        for word in ref_words]

    return lexicon, ref_words

'''lexicon, ref_words = generate_reference_phoneme(transcript)
reference_phoneme =' '.join([phon for w, phon in lexicon])
reference_phoneme'''

"""# aligns the expected and observed phonemes (useful in extracting where the error occured (word level and phoneme level))
## in : transcript, expected phonemes, actual phonemes, out : an aligned sequence for each
"""

from sequence_align.pairwise import hirschberg, needleman_wunsch

def align_sequences(reference_phoneme, phoneme,transcript):
  seq_a = reference_phoneme
  seq_b = list(phoneme.replace(' ',''))

  # recorded_phoneme['text']
  aligned_seq_a, aligned_seq_b = needleman_wunsch(
      seq_a,
      seq_b,
      match_score=1.0,
      mismatch_score=-1.0,
      indel_score=-1.0,
      gap="_",
  )
  aligned_reference_seq = ''.join(aligned_seq_a)
  aligned_recorded_seq = ''.join(aligned_seq_b)

  print('Reference Text: ', transcript)
  print('Reference Phoneme:',aligned_reference_seq)
  print('Recorded Phoneme: ', aligned_recorded_seq)

  return aligned_reference_seq, aligned_recorded_seq # Return the aligned sequences

"""# splits the sequence to a list of tuples (word, expected phoneme, actual phoneme)"""

import re

def find_word_start_positions(reference_sequence):
    # Split the sequence into words based on spaces
    words = reference_sequence.split()
    # Initialize a list to store the start positions
    start_positions = []
    # Initialize the current position
    current_position = 0
    # Iterate over the words
    for word in words:
        # Add the current position to the start positions list
        start_positions.append(current_position)
        # Increment the current position by the length of the word plus 1 (for the space)
        current_position += len(word) + 1
    return start_positions

def split_recorded_sequence(recorded_sequence, start_positions):
    # Initialize a list to store the split words
    split_words = []
    # Iterate over the start positions
    for i in range(len(start_positions)):
        # Get the start position
        start = start_positions[i]
        # If it's the last word, get the end position as the length of the sequence
        if i == len(start_positions) - 1:
            end = len(recorded_sequence)
        # Otherwise, get the end position as the start position of the next word
        else:
            end = start_positions[i + 1]
        # Extract the word from the recorded sequence
        word = recorded_sequence[start:end]
        # Add the word to the list
        split_words.append(word)
    return split_words

'''# recorded_sequence = "aɪ_hoːp_ðeɪ_hɛv_maɪ_fiːv__rədbrænd_aɪl_biː_bæk_su_n__tʊ_pliːz_w_iːdfoː__miː_"
ref_start_positions = find_word_start_positions(''.join(aligned_reference_seq))

# split recorded based on the reference start positions
rec_split_words = split_recorded_sequence(''.join(aligned_recorded_seq), ref_start_positions)
rec_split_words = [re.sub('( |\\_)$','',w) for w in rec_split_words]

# split ref based on the reference start positions
ref_split_words = split_recorded_sequence(''.join(aligned_reference_seq), ref_start_positions)
ref_split_words = [re.sub('(\\_| )$','',w) for w in ref_split_words]

# print('Reference Text: ',reference_text)
# print('(word, reference_phoneme, recorded_phoneme)',list(zip(ref_words, ref_split_words, rec_split_words)))
word_comparision_list = list(zip(ref_words, ref_split_words, rec_split_words))
word_comparision_list'''

"""# loads the llm"""

from unsloth import FastLanguageModel


model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen3-14B",
    max_seq_length = 15000,   # Context length - can be longer, but uses more memory
    load_in_4bit = True,     # 4bit uses much less memory
    load_in_8bit = False,    # A bit more accurate, uses 2x memory
    full_finetuning = False, # We have full finetuning now!
    # token = "hf_...",      # use one if using gated models
)

# Add a default chat template if it's not set
if tokenizer.chat_template is None:
    tokenizer.chat_template = "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"

"""# LLM FEEDBACK"""

import re
from transformers import TextStreamer




def analyze_pronunciation_with_qwen( transcript, word_comparision_list,rubic, model,fluency,gop_score, tokenizer, max_new_tokens=32):
  """
  For each word, send a prompt to Qwen 3 with audio, transcript, expected and actual phonemes.
  Output: If a word has a phoneme error, output only the word and the single incorrect phoneme (no explanation). If the word is correct, output only '1'.
  """
  prompt = f"""
  You are a strict speaking evaluator for the IELTS exam. Given the following information:
  - Transcript: {transcript}
  - Phonemes : (word, expected phoneme, observed phoneme) : {word_comparision_list}
  give feedback on the pronounciation, Grammatical range and accuracy, and Lexical resource of the speaker, for each of the 3 cretiria output only a sentece of feedback, your output should be in the form of :
  Fluency : Your fluency level is {fluency} with an avg GOP of {gop_score:.2f}
  Pronounciation : very short feedback
  Grammatical range and accuracy : feedback (do not mention anything about pronounciation here)
  Lexical resource of the speaker : feedback (do not mention anything about pronounciation here)
  Follow the IELTS speaking descriptors to give the speaker a very accurate feedback
  Pronounciation rubic is {rubic["Pronunciation"]}
  Grammatical range and accuracy is {rubic["Grammatical range and accuracy"]}
  Lexical resource is {rubic["Lexical resource"]}
         """
  messages = [{
      "role": "user",
      "content": prompt
  }]
  text = tokenizer.apply_chat_template(
      messages,
      tokenize=False,
      add_generation_prompt=True,
      enable_thinking=False,
  )
  
  inputs = tokenizer(text, return_tensors="pt").to("cuda")
  output_ids = model.generate(
      **inputs,
      max_new_tokens=max_new_tokens,
      temperature=0.7, top_p=0.8, top_k=20,
  )
  
  # Get only the newly generated tokens
  reply_ids = output_ids[0, inputs["input_ids"].size(1):]
  feedback = tokenizer.decode(reply_ids, skip_special_tokens=True).strip()
  
  return feedback

def evaluate_grammar_with_qwen(
    user_text: str,
    rubic: dict,
    model,
    tokenizer,
    memory: dict,
    max_new_tokens: int = 64,
):
    """
    Analyze user_text for grammatical accuracy per IELTS rubric.
    • If no errors → one sentence praising correctness.
    • Else → JSON array of {location, issue, correction}.
    """
    history = "\n".join(f"{role}: {txt}" for role, txt in memory.get("history", []))
    prompt = f"""
        You are **Qwen 3**, an IELTS *grammar‑only* evaluator.

        Rubric (use **nothing else**):
        {rubic["Grammatical range and accuracy"]}

        Conversation history:
        {history if history else "[none]"}

        RULES
        ================================================================
        1. Judge **only grammar** in *User text*.
        2. Output **valid JSON** — nothing before or after it.

          a) If **no errors**:
              {{
                "result": "correct",
                "message": "<two short, upbeat sentence (5‑10 words) praising the flawless grammar and encourge the student>"
              }}
              • Invent a new praise line each run.
              • Start with a capital letter, end with "!"
              • No line breaks, no extra keys.

          b) If **errors**:
              {{
                "result": "incorrect",
                "errors": [
                  {{
                    "location": "<exact excerpt>",
                    "issue": "<brief description>",
                    "correction": "<corrected text>"
                  }}
                  {{ ...repeat per error... }}
                ]
              }}
              • Keep the shown key order.
              • No extra keys.

        3. Produce **only** the JSON object.
        ================================================================

        User text:
        \"\"\"{user_text}\"\"\"

        Return JSON now.
        """

    memory.setdefault("history", []).append(("Q", prompt))
    messages = [{"role": "user", "content": prompt}]

    # ------------------------------------------------------------
    # 2)  Build chat prompt with Qwen template
    # ------------------------------------------------------------
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )

    # ------------------------------------------------------------
    # 3)  Encode & generate (no streamer needed)
    # ------------------------------------------------------------
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    output_ids = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.3,
        top_p=0.6,
        top_k=10,
    )

    # ------------------------------------------------------------
    # 4)  Slice off the prompt tokens and decode
    # ------------------------------------------------------------
    reply_ids = output_ids[0, inputs["input_ids"].size(1):]  # keep only the answer
    feedback = tokenizer.decode(reply_ids, skip_special_tokens=True).strip()

    # ------------------------------------------------------------
    # 5)  Store reply in memory & return
    # ------------------------------------------------------------
    memory["history"].append(("A", feedback))
    return feedback

def evaluate_lexical_resource_with_qwen(
    user_text: str,
    rubic: dict,
    model,
    tokenizer,
    memory: dict,
    max_new_tokens: int = 64,
):
    """
    Analyze user_text for lexical resource per IELTS rubric.
    Returns feedback on vocabulary usage.
    """
    history = "\n".join(f"{role}: {txt}" for role, txt in memory.get("history", []))
    prompt = f"""
        You are **Qwen 3**, an IELTS *lexical resource* evaluator.

        Rubric (use **nothing else**):
        {rubic["Lexical resource"]}

        Conversation history:
        {history if history else "[none]"}

        RULES
        ================================================================
        1. Judge **only lexical resource** in *User text*.
        2. Output **valid JSON** — nothing before or after it.

          a) If **excellent vocabulary**:
              {{
                "result": "excellent",
                "message": "<two short, upbeat sentence (5‑10 words) praising the excellent vocabulary and encourage the student>"
              }}

          b) If **good vocabulary**:
              {{
                "result": "good",
                "message": "<two short, encouraging sentence (5‑10 words) praising the good vocabulary and suggest improvements>"
              }}

          c) If **needs improvement**:
              {{
                "result": "needs_improvement",
                "message": "<two short, constructive sentence (5‑10 words) suggesting vocabulary improvements>"
              }}

        3. Produce **only** the JSON object.
        ================================================================

        User text:
        \"\"\"{user_text}\"\"\"

        Return JSON now.
        """

    memory.setdefault("history", []).append(("Q", prompt))
    messages = [{"role": "user", "content": prompt}]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )

    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    output_ids = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.3,
        top_p=0.6,
        top_k=10,
    )

    reply_ids = output_ids[0, inputs["input_ids"].size(1):]
    feedback = tokenizer.decode(reply_ids, skip_special_tokens=True).strip()

    memory["history"].append(("A", feedback))
    return feedback

def speak_text_api(transcript: str) -> bytes:
    """
    Convert text to speech using the external API.
    Returns the audio bytes.
    """
    import requests
    
    response = requests.post(
        "https://f892da4fdc1b.ngrok-free.app/speak_text",
        json={"transcript": transcript},
    )
    if response.status_code != 200:
        raise Exception("Speech server failed")
    
    return response.content

def get_ielts_question(part: int, topic: str = None, memory: dict = None) -> dict:
    """
    Get the next IELTS examiner question based on part and topic.
    Returns a dictionary with question text and audio bytes.
    """
    if memory is None:
        memory = {"history": []}
    
    # Get the question from the LLM
    question = simulate_ielts_speaking_text(
        model, tokenizer, memory, part, 
        user_response=None, topic=topic, max_new_tokens=120
    )
    
    # Convert to speech
    try:
        audio_bytes = speak_text_api(question)
        return {
            "question": question,
            "audio": audio_bytes,
            "memory": memory
        }
    except Exception as e:
        # Return text only if speech fails
        return {
            "question": question,
            "audio": None,
            "memory": memory,
            "speech_error": str(e)
        }

def simulate_ielts_speaking_text(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    memory: Dict[str, List[Tuple[str, str]]],
    part: int,
    user_response: Optional[str] = None,
    topic: Optional[str] = None,
    max_new_tokens: int = 120,
) -> str:
    """Return ONLY the next examiner utterance (plain text ≤40 words, ends with '?')."""

    # ------------------------------------------------------------------
    # A. Save candidate reply (auto‑generate if blank)
    # ------------------------------------------------------------------
    if user_response is not None:
        if user_response.strip() == "":
            last_q = next((txt for role, txt in reversed(memory.get("history", [])) if role == "Examiner"), "")
            auto_ans = _auto_candidate_reply(model, tokenizer, last_q)
            memory.setdefault("history", []).append(("Candidate", auto_ans))
        else:
            memory.setdefault("history", []).append(("Candidate", user_response))

    # ------------------------------------------------------------------
    # B. Part‑specific instruction blocks
    # ------------------------------------------------------------------
    p1_q_count = memory.get("p1_q_count", 0)

    if part == 1:
        timing_line = "Part 1 lasts 4 ‑ 5 minutes."
        if p1_q_count < 2:
            task_lines = [
                "Ask ONE open‑ended question on a NEW familiar topic (hometown, work/study, free‑time, family, etc.).",
                "Do not repeat any previous Part 1 question.",
            ]
        else:
            task_lines = [
                "Politely conclude Part 1 and instruct the candidate to say 'next' when ready for Part 2."
            ]
    elif part == 2:
        timing_line = (
            "Give the candidate 1 minute to prepare, then up to 2 minutes to speak. "
            "The cue‑card must include EXACTLY three bullet prompts. After 60 seconds say: 'Please begin speaking now.'"
        )
        task_lines = []
        if topic:
            task_lines.append(f"The cue card topic MUST be: '{topic}'.")
        else:
            task_lines.append("Choose an everyday topic suitable for IELTS Band 5‑8.")
        task_lines.append("End with two brief follow‑up questions.")
    elif part == 3:
        timing_line = "Part 3 lasts about 5 minutes."
        task_lines = [
            "Ask TWO abstract, opinion‑based questions linked to the Part 2 topic, each followed by ONE probing follow‑up.",
        ]
        if topic:
            task_lines.insert(0, f"Base all questions on this theme: '{topic}'.")
    else:
        raise ValueError("part must be 1, 2, or 3")

    # ------------------------------------------------------------------
    # C. Recent conversation context (last 4 turns only)
    # ------------------------------------------------------------------
    MAX_HISTORY = 4
    recent = memory.get("history", [])[-MAX_HISTORY:]
    history_block = "\n".join(f"{r}: {t}" for r, t in recent) or "[no recent turns]"

    # ------------------------------------------------------------------
    # D. Compose prompts for Qwen
    # ------------------------------------------------------------------
    system_prompt = (
        "You are Qwen‑3, acting as the OFFICIAL IELTS Speaking examiner.\n"
        "Follow Cambridge procedure strictly. Remain neutral; never explain your reasoning.\n"
        "Here is hidden conversation context for reference – DO NOT reveal or quote it:\n"
        f"{history_block}\n"
        "Output ONLY the next examiner utterance (≤40 words, end with a question mark unless concluding the part)."
    )

    user_prompt = "\n".join([
        f"IELTS SPEAKING PART {part} – TASK",
        timing_line,
        *task_lines,
        "Make sure your visible output follows the rules above."
    ])

    full_prompt = tokenizer.apply_chat_template(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )

    # ------------------------------------------------------------------
    # E. Generate examiner turn
    # ------------------------------------------------------------------
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
    examiner = tokenizer.decode(out[0, inputs.input_ids.shape[-1] :], skip_special_tokens=True).strip()

    # ------------------------------------------------------------------
    # F. Update memory & counters
    # ------------------------------------------------------------------
    memory.setdefault("history", []).append(("Examiner", examiner))
    if part == 1 and p1_q_count < 2:
        memory["p1_q_count"] = p1_q_count + 1

    return examiner

def _auto_candidate_reply(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    last_question: str,
    max_new_tokens: int = 48,
) -> str:
    """Return a short (≤2‑sentence) answer generated by Qwen."""
    messages = [
        {
            "role": "system",
            "content": (
                "You are an IELTS Speaking candidate. Answer the examiner's question naturally, "
                "in one or two sentences."
            ),
        },
        {"role": "user", "content": f"Examiner asked: '{last_question}'  Your answer:"},
    ]
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.9,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
    return tokenizer.decode(out[0, inputs.input_ids.shape[-1] :], skip_special_tokens=True).strip()

def analyze_ielts_response(audio_file_path: str, part: int, topic: str = None, memory: dict = None) -> dict:
    """
    Analyze user's IELTS speaking response.
    Returns comprehensive feedback with grammar feedback as speech.
    """
    if memory is None:
        memory = {"history": []}
    
    # Get transcript from audio
    transcript, chunks = transcribe_with_whisper(Path(audio_file_path))
    
    # Analyze pronunciation
    _ = audio_to_phoneme(Path(audio_file_path), return_logits=True)
    pred_ph = audio_to_phoneme(Path(audio_file_path), return_logits=False)
    gop, flu_num = audio_to_phoneme_gop(Path(audio_file_path))
    
    lexicon, ref_words = generate_reference_phoneme(transcript)
    expected = " ".join(p for _, p in lexicon)
    al_ref, al_rec = align_sequences(expected, pred_ph, transcript)
    ref_start = find_word_start_positions("".join(al_ref))
    rec_split = [re.sub(r"[\s_]+$", "", w)
                 for w in split_recorded_sequence("".join(al_rec), ref_start)]
    ref_split = [re.sub(r"[\s_]+$", "", w)
                 for w in split_recorded_sequence("".join(al_ref), ref_start)]
    
    comp = list(zip(ref_words, ref_split, rec_split))
    
    # Get pronunciation feedback
    pronunciation_feedback = analyze_pronunciation_with_qwen(
        transcript, comp, irubic, model, flu_num, gop, tokenizer, max_new_tokens=150
    )
    
    # Get grammar feedback
    grammar_feedback_json = evaluate_grammar_with_qwen(
        transcript, irubic, model, tokenizer, memory, max_new_tokens=64
    )
    
    # Parse and reorganize grammar feedback for TTS
    try:
        grammar_data = json.loads(grammar_feedback_json)
        if grammar_data.get("result") == "correct":
            grammar_feedback = grammar_data.get("message", "Good grammar!")
        else:
            # Reorganize grammar feedback for better TTS
            grammar_feedback = reorganize_grammar_feedback(grammar_feedback_json)
    except:
        grammar_feedback = grammar_feedback_json
    
    # Get lexical resource feedback
    lexical_feedback_json = evaluate_lexical_resource_with_qwen(
        transcript, irubic, model, tokenizer, memory, max_new_tokens=64
    )
    
    try:
        lexical_data = json.loads(lexical_feedback_json)
        lexical_feedback = lexical_data.get("message", "Good vocabulary usage!")
    except:
        lexical_feedback = lexical_feedback_json
    
    # Convert grammar feedback to speech
    try:
        grammar_audio = speak_text_api(grammar_feedback)
    except Exception as e:
        grammar_audio = None
        grammar_speech_error = str(e)
    
    # Get next question
    next_question_data = get_ielts_question(part, topic, memory)
    
    return {
        "transcript": transcript,
        "gop_score": float(gop),
        "fluency": "High fluency" if gop > 0.9 else "Moderate fluency" if gop > 0.75 else "Low fluency",
        "grammar_feedback": grammar_feedback,
        "grammar_audio": grammar_audio,
        "lexical_feedback": lexical_feedback,
        "next_question": next_question_data.get("question"),
        "next_question_audio": next_question_data.get("audio"),
        "memory": memory,
        "part": part,
        "topic": topic
    }

"""IELTS Speaking simulation helper – all LLM‑driven
----------------------------------------------------------
Utilities
---------
1. _auto_candidate_reply():  If the candidate hits Enter, Qwen fabricates a brief, natural answer.
2. simulate_ielts_speaking_text():  Produces the next examiner turn for Parts 1‑3 – every question
   or cue card is generated by the LLM (no hard‑coded questions).

Conversation state
------------------
`memory` is a dict with:
    memory["history"] : list[tuple(role, text)]  – running transcript
    memory["p1_q_count"] : int                    – how many Part‑1 questions already asked
Only the **last 4** turns are given to the model each call.
"""
from __future__ import annotations
from typing import Dict, List, Tuple, Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# -----------------------------------------------------------------------
# 0) Helper: fabricate an answer if the candidate skips a reply
# -----------------------------------------------------------------------

def _auto_candidate_reply(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    last_question: str,
    max_new_tokens: int = 48,
) -> str:
    """Return a short (≤2‑sentence) answer generated by Qwen."""
    messages = [
        {
            "role": "system",
            "content": (
                "You are an IELTS Speaking candidate. Answer the examiner's question naturally, "
                "in one or two sentences."
            ),
        },
        {"role": "user", "content": f"Examiner asked: '{last_question}'  Your answer:"},
    ]
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.9,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
    return tokenizer.decode(out[0, inputs.input_ids.shape[-1] :], skip_special_tokens=True).strip()

# -----------------------------------------------------------------------
# 1)  Main: produce the next examiner turn (all parts LLM‑driven)
# -----------------------------------------------------------------------

def simulate_ielts_speaking_text(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    memory: Dict[str, List[Tuple[str, str]]],
    part: int,
    user_response: Optional[str] = None,
    topic: Optional[str] = None,
    max_new_tokens: int = 120,
) -> str:
    """Return ONLY the next examiner utterance (plain text ≤40 words, ends with '?')."""

    # ------------------------------------------------------------------
    # A. Save candidate reply (auto‑generate if blank)
    # ------------------------------------------------------------------
    if user_response is not None:
        if user_response.strip() == "":
            last_q = next((txt for role, txt in reversed(memory.get("history", [])) if role == "Examiner"), "")
            auto_ans = _auto_candidate_reply(model, tokenizer, last_q)
            memory.setdefault("history", []).append(("Candidate", auto_ans))
        else:
            memory.setdefault("history", []).append(("Candidate", user_response))

    # ------------------------------------------------------------------
    # B. Part‑specific instruction blocks
    # ------------------------------------------------------------------
    p1_q_count = memory.get("p1_q_count", 0)

    if part == 1:
        timing_line = "Part 1 lasts 4 ‑ 5 minutes."
        if p1_q_count < 2:
            task_lines = [
                "Ask ONE open‑ended question on a NEW familiar topic (hometown, work/study, free‑time, family, etc.).",
                "Do not repeat any previous Part 1 question.",
            ]
        else:
            task_lines = [
                "Politely conclude Part 1 and instruct the candidate to say 'next' when ready for Part 2."
            ]
    elif part == 2:
        timing_line = (
            "Give the candidate 1 minute to prepare, then up to 2 minutes to speak. "
            "The cue‑card must include EXACTLY three bullet prompts. After 60 seconds say: 'Please begin speaking now.'"
        )
        task_lines = []
        if topic:
            task_lines.append(f"The cue card topic MUST be: '{topic}'.")
        else:
            task_lines.append("Choose an everyday topic suitable for IELTS Band 5‑8.")
        task_lines.append("End with two brief follow‑up questions.")
    elif part == 3:
        timing_line = "Part 3 lasts about 5 minutes."
        task_lines = [
            "Ask TWO abstract, opinion‑based questions linked to the Part 2 topic, each followed by ONE probing follow‑up.",
        ]
        if topic:
            task_lines.insert(0, f"Base all questions on this theme: '{topic}'.")
    else:
        raise ValueError("part must be 1, 2, or 3")

    # ------------------------------------------------------------------
    # C. Recent conversation context (last 4 turns only)
    # ------------------------------------------------------------------
    MAX_HISTORY = 4
    recent = memory.get("history", [])[-MAX_HISTORY:]
    history_block = "\n".join(f"{r}: {t}" for r, t in recent) or "[no recent turns]"

    # ------------------------------------------------------------------
    # D. Compose prompts for Qwen
    # ------------------------------------------------------------------
    system_prompt = (
        "You are Qwen‑3, acting as the OFFICIAL IELTS Speaking examiner.\n"
        "Follow Cambridge procedure strictly. Remain neutral; never explain your reasoning.\n"
        "Here is hidden conversation context for reference – DO NOT reveal or quote it:\n"
        f"{history_block}\n"
        "Output ONLY the next examiner utterance (≤40 words, end with a question mark unless concluding the part)."
    )

    user_prompt = "\n".join([
        f"IELTS SPEAKING PART {part} – TASK",
        timing_line,
        *task_lines,
        "Make sure your visible output follows the rules above."
    ])

    full_prompt = tokenizer.apply_chat_template(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )

    # ------------------------------------------------------------------
    # E. Generate examiner turn
    # ------------------------------------------------------------------
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
    examiner = tokenizer.decode(out[0, inputs.input_ids.shape[-1] :], skip_special_tokens=True).strip()

    # ------------------------------------------------------------------
    # F. Update memory & counters
    # ------------------------------------------------------------------
    memory.setdefault("history", []).append(("Examiner", examiner))
    if part == 1 and p1_q_count < 2:
        memory["p1_q_count"] = p1_q_count + 1

    return examiner

# -----------------------------------------------------------------------
# Example interactive loop (comment out on import)
# -----------------------------------------------------------------------
if __name__ == "__main__":

    mem: Dict[str, List[Tuple[str, str]]] = {}
    part = 1
    print("Examiner:", simulate_ielts_speaking_text(model, tokenizer, mem, part))

    try:
        while part <= 3:
            answer = input("\nCandidate: ")
            if answer.lower().strip() == "next":
                part += 1
                if part > 3:
                    break
                print("\nExaminer:", simulate_ielts_speaking_text(model, tokenizer, mem, part))
            else:
                print(
                    "\nExaminer:",
                    simulate_ielts_speaking_text(
                        model, tokenizer, mem, part, user_response=answer
                    ),
                )
    except KeyboardInterrupt:
        print("\nSession terminated.")

!ngrok config add-authtoken 30NkU4x5BjWRCW1VsFQ5zNw0h8q_4YtQ6sjZfds535NfAoyAV

#!/usr/bin/env python3
# -------------------------------------------------------------------
# FastAPI + ngrok  • Pronunciation‑AI server  (prints → feedback fixed)
# -------------------------------------------------------------------
import io, contextlib, uuid, zipfile, difflib, re, json
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import requests

import librosa, numpy as np, soundfile as sf, torch
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TextStreamer
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse, FileResponse, Response
from pyngrok import ngrok
import uvicorn, nest_asyncio


nest_asyncio.apply()

# ---------------- configuration ------------------------------------
PRE_MS, POST_MS = 2000, 1000          # context around each clip
TMP_DIR = Path("sessions"); TMP_DIR.mkdir(exist_ok=True)

# ---------------- Whisper pipeline ---------------------------------
ASR = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-small.en",
    device=0 if torch.cuda.is_available() else -1,
    chunk_length_s=30,
    return_timestamps="word",
    ignore_warning=True,
)

def transcribe_with_whisper(wav: Path, fallback_ms=300):
    out = ASR(str(wav), batch_size=8)
    transcript = out["text"].strip()
    chunks, prev_end = [], 0
    for c in out["chunks"]:
        w, (ts, te) = c["text"].strip().lower(), c["timestamp"]
        if ts is None: ts = prev_end / 1000
        if te is None: te = ts + fallback_ms / 1000
        s_ms, e_ms = int(ts*1000), int(te*1000)
        prev_end = e_ms
        chunks.append((w, s_ms, e_ms))
    return transcript, chunks

# ---------------- word‑matching helpers ----------------------------
def fuzzy_eq(a, b, cut=0.83):  # Levenshtein ratio
    return difflib.SequenceMatcher(None, a, b).ratio() >= cut

def locate_word(tgt: str, chunks: List, idx: int):
    for i in range(idx, len(chunks)):
        w, s, e = chunks[i]
        if w == tgt or fuzzy_eq(w, tgt):
            return i, s, e
    for i in range(idx, len(chunks)-1):
        w1, s1, _  = chunks[i]
        w2, _,  e2 = chunks[i+1]
        if w1 + w2 == tgt or fuzzy_eq(w1 + w2, tgt):
            return i+1, s1, e2
    return None, None, None

# ---------------- clip + zip helper --------------------------------
def save_error_clips(wav: Path, wrongs, sdir: Path):
    y, sr = librosa.load(wav, sr=None, mono=True)
    total_ms = int(len(y)/sr*1000)
    cdir = sdir / "error_clips"; cdir.mkdir(exist_ok=True)

    segs, meta = [], []
    for i, (w, _, _, ws, we) in enumerate(wrongs, 1):
        cs, ce = max(0, ws-PRE_MS), min(total_ms, we+POST_MS)
        seg = y[int(cs/1000*sr): int(ce/1000*sr)]
        segs.append(seg)
        sf.write(cdir / f"{i:02d}_{w}.wav", seg, sr)
        meta.append((w, cs, ce))

    err_wav = sdir / "errors.wav"
    if segs:
        sf.write(err_wav, np.concatenate(segs), sr)

    zpath = sdir / "pronunciation_errors.zip"
    with zipfile.ZipFile(zpath, "w", zipfile.ZIP_DEFLATED) as zf:
        for f in cdir.rglob("*"):
            zf.write(f, arcname=f.relative_to(sdir))
        if err_wav.exists():
            zf.write(err_wav, arcname=err_wav.name)
    return meta, zpath

# ---------------- FastAPI app --------------------------------------
app = FastAPI(title="Pronunciation‑AI")

# Global variables for model and tokenizer
model = None
tokenizer = None
irubic = None

@app.on_event("startup")
async def startup_event():
    """Initialize models when the app starts"""
    initialize_models()

def initialize_models():
    """Initialize the Qwen model and tokenizer"""
    global model, tokenizer, irubic
    
    if model is None:
        model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2.5-7B-Instruct",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen2.5-7B-Instruct",
            trust_remote_code=True
        )
        
        # Initialize IELTS rubric
        irubic = {
            "Pronunciation": "Band 9: Uses a full range of pronunciation features with precision and subtlety. Band 8: Uses a wide range of pronunciation features. Band 7: Shows all the positive features of Band 6 and some, but not all, of the positive features of Band 8. Band 6: Uses a range of pronunciation features with mixed control. Band 5: Shows all the positive features of Band 4 and some, but not all, of the positive features of Band 6. Band 4: Uses a limited range of pronunciation features. Band 3: Attempts to control basic pronunciation features. Band 2: Speech is often unintelligible. Band 1: No communication possible.",
            "Grammatical range and accuracy": "Band 9: Uses a full range of structures naturally and appropriately. Band 8: Uses a wide range of structures flexibly and accurately. Band 7: Uses a range of complex structures with some flexibility. Band 6: Uses a mix of simple and complex structures, but with limited flexibility. Band 5: Uses a limited range of structures, attempting complex sentences but with limited success. Band 4: Uses only a very limited range of structures with only rare use of subordinate clauses. Band 3: Attempts sentence forms but errors in grammar and punctuation predominate. Band 2: Cannot use sentence forms except in memorised phrases. Band 1: Cannot use sentence forms at all.",
            "Lexical resource": "Band 9: Uses vocabulary with full flexibility and precision in all topics. Band 8: Uses a wide vocabulary resource readily and flexibly to convey precise meaning. Band 7: Uses vocabulary resource flexibly to discuss a variety of topics. Band 6: Has a wide enough vocabulary to discuss topics at length. Band 5: Manages to talk about familiar and unfamiliar topics but uses vocabulary with limited flexibility. Band 4: Is able to talk about familiar topics but can only convey basic meaning on unfamiliar topics. Band 3: Conveys and understands only general meaning in very familiar situations. Band 2: Only produces isolated words or memorised phrases. Band 1: No communication possible."
        }

@app.post("/analyze")
async def analyze(audio: UploadFile = File(...)):
    if not audio.filename.lower().endswith(".wav"):
        raise HTTPException(400, "Only .wav accepted")

    sid = uuid.uuid4().hex
    sdir = TMP_DIR / sid; sdir.mkdir()
    wav = sdir / "input.wav"; wav.write_bytes(await audio.read())

    # ---------- pipeline ----------
    _ = audio_to_phoneme(wav, return_logits=True)
    pred_ph = audio_to_phoneme(wav, return_logits=False)
    gop, flu_num = audio_to_phoneme_gop(wav)
    transcript, chunks = transcribe_with_whisper(wav)

    lexicon, ref_words = generate_reference_phoneme(transcript)
    expected = " ".join(p for _, p in lexicon)
    al_ref, al_rec = align_sequences(expected, pred_ph, transcript)
    ref_start = find_word_start_positions("".join(al_ref))
    rec_split = [re.sub(r"[\s_]+$", "", w)
                 for w in split_recorded_sequence("".join(al_rec), ref_start)]
    ref_split = [re.sub(r"[\s_]+$", "", w)
                 for w in split_recorded_sequence("".join(al_ref), ref_start)]

    comp = list(zip(ref_words, ref_split, rec_split))
    wrongs, idx = [], 0
    for w, rp, rc in comp:
        if rp == rc: continue
        pos, s_ms, e_ms = locate_word(w.lower(), chunks, idx)
        if pos is not None:
            wrongs.append((w, rp, rc, s_ms, e_ms))
            idx = pos + 1

    clip_meta, zpath = save_error_clips(wav, wrongs, sdir)

    flu_label = ("High fluency" if gop > 0.9 else
                 "Moderate fluency" if gop > 0.75 else
                 "Low fluency")

    # ---------- capture Qwen output ----------
    try:
        buf = io.StringIO()
        with contextlib.redirect_stdout(buf):
            out = analyze_pronunciation_with_qwen(
                transcript, comp, irubic,
                model, flu_num, gop, tokenizer,
                max_new_tokens=150,
            )
        feedback = (out if out else buf.getvalue()).strip()
        if not feedback:
            feedback = "⚠️  Generation returned nothing."
    except Exception as err:
        print("!! Qwen failed:", err)
        feedback = "⚠️  Could not generate feedback due to an internal error."

    mis_words = [
        {"word": w, "ref": rp, "rec": rc,
         "clip_start_ms": cs, "clip_end_ms": ce}
        for (w, rp, rc, _, _), (_, cs, ce) in zip(wrongs, clip_meta)
    ]

    return JSONResponse({
        "session_id": sid,
        "transcript": transcript,
        "gop_score": float(gop),
        "fluency": flu_label,
        "feedback": feedback,
        "mispronounced": mis_words,
        "zip_url": f"/download/{sid}"
    })

@app.get("/download/{sid}")
async def download_zip(sid: str):
    z = TMP_DIR / sid / "pronunciation_errors.zip"
    if not z.exists():
        raise HTTPException(404, "Not found")
    return FileResponse(z, media_type="application/zip",
                        filename="pronunciation_errors.zip")

# IELTS Speaking Simulation Routes
@app.post("/get_ielts_question")
async def get_ielts_question_route(request: dict):
    """Get the next IELTS examiner question"""
    try:
        # Initialize models if not already done
        if model is None or tokenizer is None:
            initialize_models()
        
        if model is None or tokenizer is None:
            raise HTTPException(status_code=500, detail="Models failed to initialize")
        
        part = request.get("part", 1)
        topic = request.get("topic")
        memory = request.get("memory", {"history": []})
        
        # Get question data
        question_data = get_ielts_question(part, topic, memory)
        
        return JSONResponse({
            "question": question_data["question"],
            "memory": question_data["memory"],
            "part": part,
            "topic": topic
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze_ielts_response")
async def analyze_ielts_response_route(audio: UploadFile = File(...), part: int = 1, topic: str = None, memory: str = None):
    """Analyze user's IELTS speaking response"""
    try:
        # Initialize models if not already done
        if model is None:
            initialize_models()
        
        # Save uploaded audio temporarily
        temp_audio_path = TMP_DIR / f"temp_response_{uuid.uuid4().hex}.wav"
        temp_audio_path.write_bytes(await audio.read())
        
        # Parse memory if provided
        memory_dict = {}
        if memory:
            try:
                memory_dict = json.loads(memory)
            except:
                memory_dict = {"history": []}
        
        # Analyze response
        result = analyze_ielts_response(str(temp_audio_path), part, topic, memory_dict)
        
        # Clean up temporary file
        temp_audio_path.unlink()
        
        # Convert audio bytes to base64 for JSON response
        import base64
        grammar_audio_b64 = None
        next_question_audio_b64 = None
        
        if result.get("grammar_audio"):
            grammar_audio_b64 = base64.b64encode(result["grammar_audio"]).decode('utf-8')
        
        if result.get("next_question_audio"):
            next_question_audio_b64 = base64.b64encode(result["next_question_audio"]).decode('utf-8')
        
        return JSONResponse({
            "transcript": result["transcript"],
            "gop_score": result["gop_score"],
            "fluency": result["fluency"],
            "grammar_feedback": result["grammar_feedback"],
            "grammar_audio": grammar_audio_b64,
            "lexical_feedback": result["lexical_feedback"],
            "next_question": result["next_question"],
            "next_question_audio": next_question_audio_b64,
            "memory": result["memory"],
            "part": result["part"],
            "topic": result["topic"]
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/speak_text")
async def speak_text_route(request: dict):
    """Convert text to speech"""
    try:
        transcript = request.get("transcript", "")
        if not transcript:
            raise HTTPException(status_code=400, detail="No transcript provided")
        
        audio_bytes = speak_text_api(transcript)
        return Response(content=audio_bytes, media_type="audio/wav")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def reorganize_grammar_feedback(grammar_feedback):
    """
    Reorganize grammar feedback JSON into natural language for TTS
    """
    try:
        # Try to parse as JSON
        if isinstance(grammar_feedback, str):
            import json
            data = json.loads(grammar_feedback)
        else:
            data = grammar_feedback
        
        # Check if it's the expected format
        if isinstance(data, dict) and 'result' in data and 'errors' in data:
            if data['result'] == 'correct':
                return "Your grammar is correct. Well done!"
            
            # Reorganize errors into natural language
            feedback_parts = []
            for error in data.get('errors', []):
                location = error.get('location', '')
                issue = error.get('issue', '')
                correction = error.get('correction', '')
                
                # Create natural language feedback
                if location and correction:
                    feedback_parts.append(f"The Mistake: {location}. {issue}. The Correction: {correction}")
                elif issue:
                    feedback_parts.append(f"Grammar issue: {issue}")
            
            if feedback_parts:
                return "Grammar feedback: " + ". ".join(feedback_parts)
            else:
                return "Some grammar issues were found. Please review your response."
        
        # If not JSON format, return as is
        return grammar_feedback
        
    except (json.JSONDecodeError, TypeError):
        # If not valid JSON, return as is
        return grammar_feedback

# ---------------- run + ngrok ------------------------------------
if __name__ == "__main__":
    public = ngrok.connect(8000).public_url
    print(f"🚀 Public endpoint: {public}/analyze")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")